:docinfo:

:slesProdVersion: 12

= SAP NetWeaver Enqueue Replication 1 High Availability Cluster - Setup Guide for SAP NetWeaver 7.40 and 7.50 on Alibaba Cloud

//:toc:
include::Variables_HA740_AliCoud.adoc[]

////
Weitere TODOs:
TODO use the correct include files and include "places" for the common files to be compatible for the documentation team
TODO maybe the whole setup could be divided in two parts: Enqueue Replication
cluster which is the core part, and the auxiliary pieces which might come from somewhere?
TODO: PRIO1: Document that autostart for SAP instances must be switched-off
REJC p.2 either adding the third node to the cluster -> (would change the basic setup)
TODO p.4 https://blogs.sap.com/2014/05/08/using-sapvendorclusterconnector-for-interaction-between-cluster-framework-and-sapstartsrv/comment-page-1/ -> (check link)
TODO p.7 (somewhere explain that the D02 and DVEBMGS01 on the DB host just to simplify this lab environment)
TODO p.7,8 (only short hostnames in /etc/hosts?)
TODO p.13 TODO/TBD ->
TODO p.14 File /usr/sap/HA1/SYS/profile/HA1_ASCS00_sapha1as. -> (font)
TODO p.14 File /usr/sap/HA1/SYS/profile/HA1_ERS10_sapha1er. -> (font)
TODO p.17 TODO/TBD ->
TODO p.17 <nul> -> refer to sle-ha quickstart guide on our webpage
TODO p.18 <nul> -> (run ha-cluter-join or do something to enable cluster?)
TODO p.18 <nul> -> some notes on adpating resulting config, depending on environment
TODO p.24 /usr/sap/HA1/SYS/exe/uc/linuxx86_64/sapcontrol -> # linuxx86_64/sapcontrol
TODO p.25 As ha1adm. -> (font)
TODO p.25,26,27,28,29 sapcontrol -nr -> # sapcontrol -nr
TODO p.31 in HA-Umgebungen -> in HA environments (links without "D"?)
TODO p.32 ... -> TODO/TBD
TODO p.32 -> SLE-HA release notes https://www.suse.com/releasenotes/x86_64/SLE-HA/12-SP2/
TODO p.32 SLE-HA quick setup guide
TODO p.32 SLE-HA product docu
TODO p.32 -> TODO SLES-for-SAP release note
TODO p.32 -> TODO product docu
TODO -> NFS SAP layout for instance
TODO: non line break with in command or variables
TODO: table of required values
TODO: update output of command
TODO: remove AWS, rework stonith methode and filesystems
TODO: replace WAS and 00 and wasadm with variables
TODO: add required parameters, using internal marker to other chapters
////

== About this Guide

=== Introduction

{sles4sapReg} is the optimal platform to
run {sapReg} applications with high availability (HA). Together with a redundant layout
of the technical infrastructure, single points of failure can be eliminated.

{sapBSReg} is a sophisticated application platform for large enterprises
and mid-size companies. Many critical business environments require the highest
possible {sapReg} application availability.

The described cluster solution can be used for {sapReg} S/4 HANA and for
{sapReg} {sapNW}.

{sapNw} is a common stack of middleware functionality used to support the SAP
business applications. The {sapERS} constitutes application
level redundancy for one of the most crucial components of the {sapNw} stack,
the enqueue service. An optimal effect of the enqueue replication mechanism can
be achieved when combining the application level redundancy with a high
availability cluster solution as provided with {sles4sap}. The described
concept has proven its maturity over several years of productive operations for
customers of different sizes and branches.


=== Additional Documentation and Resources

Chapters in this manual contain links to additional documentation resources that
are either available on the system or on the Internet.

For the latest documentation updates, see https://documentation.suse.com/.

Numerous whitepapers, a best practices guide, and other
resources are provided at the {sles4sap} resource
library: https://www.suse.com/products/sles-for-sap/#resource .

This guide and other SAP-specific best practices documents can be downloaded from
the documentation portal at https://documentation.suse.com/sbp/all.

Here you can find guides for {SAPHANA} system replication
automation and HA scenarios for {SAPNw} and {s4hana}.

// Standard SUSE includes
=== Feedback
include::common_intro_feedback.adoc[]

//=== Documentation Conventions
//TODO work on SUSE doc standard conventions file
//include::common_intro_typografie.adoc[]

== Scope of This Document

This guide details how to:

- Plan a {sleHA} platform for {sapNw},
  including {sapERS}.
- Set up a {linux} high availability platform and perform a basic {sapNW}
  installation including {sapERS} on {sle}.
- Integrate the high availability cluster with the {sap} control framework via
  {s4sClConnector3}, as certified by {sap}.

This guide focuses on the high availability of the central services.

////
HA cluster solutions for the database and {sapNW} instances are described in the best
practice "Simple Stack" available on our landing page (see section "Additional
documentation and resources").
////

For {saphana} system replication, follow the guides for the performance- or cost-optimized scenario.

== Overview

This guide describes how to set up a pacemaker cluster using {sles4sap}
{slesProdVersion} for the Enqueue Replication scenario on Alibaba Cloud. The goal is to match
the {sapCert} certification specifications and goals.

These goals include:

- Integration of the cluster with the {SAP} start framework _sapstartsrv_ to
  ensure that maintenance procedures do not break the cluster stability
- Rolling Kernel Switch (RKS) awareness
- Standard {sap} installation to improve support processes

The updated certification {sapcert} has redefined some of the test procedures
and described new expectations how the cluster should behave in special
conditions. These changes allowed us to improve the cluster architecture and to
design it for easier usage and setup.

Shared SAP resources are on a central NFS server.

The {sap} instances themselves are installed on a shared disk to allow switching over the file
systems for proper functionality. The second need for a shared disk is that we are using the SBD
for the cluster fencing mechanism STONITH.
////
TODO: with SAP clarify for NFS layout for instance profile
////

=== Differences to Previous Cluster Architectures

The concept is different to the old stack with the master-slave architecture.
With the new certification we switch to a more simple model with primitives.
This means we have on one machine the ASCS with its own resources and on
the other machine the ERS with its own resources.


=== Three Systems for ASCS, ERS, Database and Additional SAP Instances

This guide describes the installation of a distributed {sap} system on three
systems. In this setup, only two systems are in the cluster. The database and
{sap} dialog instances could also be added to the cluster by either adding the
third node to the cluster or by installing the database on either of the
nodes. However we recommend to install the database on a separate
cluster.

NOTE: The cluster in this guide only manages the {sap} instances ASCS and ERS,
because of the focus of the {sapCert} certification.

If your database is {sapHana}, we recommend to set up the performance optimized
system replication scenario using our automation solution {sapHanaSR}. The
{sapHanaSR} automation should be set up in an own two node cluster. The setup is
described in a separate best practices document available at http://documentation.suse.com/sbp/all.
In case of using ASE database together with an HADR setup there is an example in this document.

.Three systems for the certification setup
image::sles4sap_nw740_3nodes.svg[SVG]

.Clustered machines

*    one machine ({myNode1}) for ASCS
**    Hostname:    {myVipNAscs}

*    one machine ({myNode2}) for ERS
**    Hostname:   {myVipNErs}

.Non-Clustered machine

*    one machine ({myNode3}) for DB and DI
////
**    Hostname:   {myVipNDb}
**    Hostname:   {myVipNPas}
**    Hostname:   {myVipNDSec}
////

=== High Availability for the Database

Depending on your needs you can also increase the availability of the database if your
database is not already highly available by design.

==== {SapHana} System Replication

A perfect enhancement of the three node scenario described in this document is
to implement an {saphana} system replication (SR) automation.

.One cluster for central services, one for {saphana} SR
image::sles4sap_nw740_cs+hanasr.svg[SVG]

The following Databases are supported in combination with this scenario:

- SAP HANA DATABASE 1.0
- SAP HANA DATABASE 2.0

==== ASE Database Replication

The picture below show a solution for a ASE HADR setup. The ASE has his own HA mechanism which
is managed by Fault Manager.
The Fault Manager himself is a single point of failure. The implementation as integrated service or as a
separate SAP instance in the Pacemaker cluster for the central services solve this weakness.

.One cluster for the central services and the ASE database HADR solution
image::sles4sap_nw740_cs+asedb.svg[SVG]

The following Databases are supported in combination with this scenario:

- ASE16 SP03 PL07 onwards

==== Simple Stack

Another option is to implement a second cluster for a database without SR aka
"ANYDB". The cluster resource agent SAPDatabase uses the SAPHOSTAGENT to control
and monitor the database.

.One cluster for the central services and one cluster for the ANY database
image::sles4sap_nw740_cs+anydb.svg[SVG]

.The following OS / Databases combination are examples for this scenario
[width="85%",options="header"]
|=========================================================
2+^|{sles4sap} 12
^| *Intel X86_64* ^|*POWER LITTLE ENDIAN*
|SAP HANA DATABASE 1.0  |
|SAP HANA DATABASE 2.0  |SAP HANA DATABASE 2.0
|DB2 FOR LUW 10.5|
|MaxDB 7.9|
|ORACLE 12.1|
|SAP ASE 16.0 FOR BUS. SUITE |
|=========================================================

NOTE: First version for {sapNW} on Power Little Endian is 7.50. More information about
supported combination of OS and Databases for {sapNW} can be found at the
SAP Product Availability Matrix. (https://apps.support.sap.com/sap/support/pam[SAP PAM])

=== Integration of {SapNW} into the Cluster Using the Cluster Connector

The integration of the HA cluster through the SAP control framework using the
{s4sClConnector} is of special interest. The {SAPSTARTSRV} controls {sap} instances since
{sap} Kernel versions 6.40. One of the classical problems running
{sap} instances in a highly available environment is the following: If an {sap}
administrator changes the status (start/stop) of an {sap} instance without using
the interfaces provided by the cluster software, the cluster framework will
detect that as an error status and will bring the {sap} instance into the old
status by either starting or stopping the {sap} instance. This can result in
very dangerous situations if the cluster changes the status of an {sap} instance
during some {sap} maintenance tasks. This new updated solution enables the central component
{SAPSTARTSRV} to report state changes to the cluster software, and therefore avoids the
previously described dangerous situations.
(See also blog article "Using sap_vendor_cluster_connector for interaction between cluster
framework and sapstartsrv")
(https://blogs.sap.com/2014/05/08/using-sapvendorclusterconnector-for-interaction-between-cluster-framework-and-sapstartsrv/comment-page-1/).

.Cluster connector to integrate the cluster with the {sap} start framework
image::sles4sap_clusterconnector.svg[SVG]

NOTE: For this scenario we are using an updated version of the {s4sClConnector3}.
This version implements the API version 3 for the communication between the cluster
framework and the {sapstartsrv}.

The new version of the {s4sClConnector3} now allows to start, stop and 'move'
an {sap} instance. The integration between the cluster software and the
{sapstartsrv} also implements the option to run checks of the HA setup using either the
command line tool sapcontrol or the {SAP} management consoles ({SAP} MMC or
{sap} MC).

=== Instances and Services
??? more details what we need for the setup ???

=== Disks and Partitions

For all {sap} file systems beside the file systems on NFS we are using XFS.

==== Shared Disk for Cluster ASCS and ERS

////
//TODO: using NFS disk here!!!!!
The disk for the ASCS and ERS instances need to be shared and assigned to the
cluster nodes {myNode1} and {myNode2}.

On {myNode1} prepare the file systems for the shared disk. Create three partitions on
the shared drive {myDev}:

* partition one ({myDevPartAscs}) for the first file system (10GB) formatted
with XFS
* partition two ({myDevPartErs}) for the second file system (10GB) formatted
with XFS

You could either use YaST to create partitions or using available command line
tools. The following script could be used for non-interactive setups.

[subs="attributes"]
----
# parted -s {myDev} print
# # we are on the 'correct' drive, right?
# parted -s {myDev} mklabel gpt
# parted -s {myDev} mkpart primary 1049k 10.7G
# parted -s {myDev} mkpart primary 10.7G 21.5G
# mkfs.xfs {myDevPartAscs}
# mkfs.xfs {myDevPartErs}
----

For these file systems we recommend to use plain partitions to keep the cluster
configuration as easy as possible. However you could also place these file
systems in separate volume groups. In that case you need to add further cluster
resources to control the logical volume groups. This is out of the scope of this
setup guide.

After we have partitioned the shared disk on {myNode1} we need to request a
partition table rescan on {myNode2}.

[subs="attributes"]
----
# partprobe; fdisk -l {myDev}
----

During the SAP installation we need {myMpAscs} to be mounted on {myNode1} and
{myMpErs} to be mounted on {myNode2}.

////
Create two NAS storage  via **Console->nas->File System List->Create File System->General Purpose NAS(Pay-as-you-go)**, in this example following two NAS have been created:

image::Alicloud_HA740_nas1.png[scaledwidth=100.0%]

Afterwards, please execute below command to mount the created NAS storage to {myNode1}:
[subs="attributes"]
----
# mkdir {myMpAscs}
# mount {myDevPartAscs} {myMpAscs}
----

Afterwards, please execute below command to mount the created NAS storage to {myNode2}:
[subs="attributes"]
----
# mkdir {myMpErs}
# mount {myDevPartErs} {myMpErs}
----

The mount points are like this:

* {myNode1}:   
** {myDevPartAscs}   {myMpAscs}
* {myNode2}:   
** {myDevPartErs}   {myMpErs}

==== Disk for DB and Dialog Instances (ASE DB Example)

The disk for the database and primary application server is assigned to
{myNode3}. In an advanced setup this disk should be shared between {myNode3}
and an optional additional node building an own cluster.
////
* partition one ({myDevPartSbd}) for SBD (7M) - not used here but a reservation
for an optional second cluster
* partition two ({myDevPartDb}) for the Database (60GB) formatted with XFS
* partition three ({myDevPartPas}) for the second file system (10GB) formatted
with XFS
* partition four ({myDevPartSec}) for the third file system (10GB)
formatted with XFS

You could either use YaST to create partitions or using available command line
tools. The following script could be used for non-interactive setups.

[subs="attributes"]
----
# parted -s {myDev} print
# # we are on the 'correct' drive, right?
# parted -s {myDev} mklabel gpt
# parted -s {myDev} mkpart primary 1049k 8388k
# parted -s {myDev} mkpart primary 8389k 60G
# parted -s {myDev} mkpart primary 60G 70G
# parted -s {myDev} mkpart primary 70G 80G
# mkfs.xfs {myDevPartDb}
# mkfs.xfs {myDevPartPas}
# mkfs.xfs {myDevPartSec}
----

.To be mounted either by OS or an optional cluster
- {myNode3}:   {myDevPartDb}   {myMpDb}

- {myNode3}:   {myDevPartPas}   {myMpPas74}

- {myNode3}:   {myDevPartSec}   {myMpSec}

NOTE:  {myInstPas750} => Since NetWeaver 7.5, the primary application server instance
directory has been renamed. (D<Instance_Number>)

////
//.NFS server
//- {myNfsSrv}:{myNFSExpPath}/{mySid}/sapmnt   /sapmnt
//
//- {myNfsSrv}:{myNFSExpPath}/{mySid}/usrsapsys /usr/sap/{mySid}/SYS
////
.Media
- {myNfsSrv}:{myNFSExpPathSapMedia740} /sapcd

or

- {myNfsSrv}:{myNFSExpPathSapMedia750} /sapcd
////

In our example, we use block storage for database storage. Please execute below command on node {myNode3}:
[subs="attributes"]
----
# mkdir {myMPDb}
# echo {myDevPartDb} {myMPDb} ext4 acl,user_xattr,noatime 1 1 >> /etc/fstab
----
////
#sapdb2: mkdir {myMPDb}
#sapdb2: echo {myDevPartDb} {myMPDb} ext4 acl,user_xattr,noatime 1 1 >> /etc/fstab
----
////

NOTE:  {myInstPas750} => Since NetWeaver 7.5, the primary application server instance
directory has been renamed. (D<Instance_Number>)

=== IP Addresses and Virtual Names

Check, if the _/etc/hosts_ contains at least the following address resolutions.
Add those entries, if they are missing.

[subs="attributes"]
----
{myIPNode1}  {myNode1}
{myIPNode2}  {myNode2}
{myIPNode3}  {myNode3}
{myVipAAscs}  {myVipNAscs}
{myVipAErs}  {myVipNErs}
----

//{myVipADb}  {myVipNDb}
//{myVipAPas}  {myVipNPas}
//{myVipADSec}  {myVipNDSec}

=== Mount Points and NFS Shares

In our setup the directory _/usr/sap_ is part of the root file system. You could
of course also create a dedicated file system for that area and mount _/usr/sap_
during the system boot. As _/usr/sap_ also contains the {sap} control file
_sapservices_ and the {saphostagent}, the directory should not be placed on a
shared file system between the cluster nodes.

We need to create the directory structure on all nodes which might be able to
run the SAP resource. The SYS directory will be on an NFS share for all nodes.

- Creating mount points and mounting NFS share on all nodes

////
.{sapNW} 7.4
==============================================
[subs="attributes"]
----
# mkdir -p /sapcd
# mkdir -p /sapmnt
# mkdir -p /usr/sap/{mySid}/{{myInstAscs},{myInstDSec},{myInstPas740},{myInstErs},SYS}
# mount -t nfs {myNfsSrv}:{myNFSExpPath}/{mySid}/sapmnt    /sapmnt
# mount -t nfs {myNfsSrv}:{myNFSExpPath}/{mySid}/usrsapsys /usr/sap/{mySid}/SYS
# mount -t nfs {myNfsSrv}:{myNFSExpPathSapMedia740} /sapcd
----
==============================================
////
.{sapNW} 7.5
==============================================
[subs="attributes"]
----
# mkdir -p /sapmnt
# mkdir -p /usr/sap/{mySid}/{{myInstAscs},{myInstPas750},{myInstDSec},{myInstErs},SYS}
# mount -t nfs {myNfsSrv}:{myNFSExpPath}    /sapmnt
# mount -t nfs {myNfsSrv}:{myDevPartSYS} /usr/sap/{mySid}/SYS
----
==============================================

- Only ASEDB:  creating mount points for the database at {myNode3}:

[subs="attributes"]
----
# mkdir -p /sybase/SSA/srsdata
----

- Only HANA: creating mount points for database at {myNode3}:

[subs="attributes"]
----
# mkdir -p /hana/{shared,data,log}
----

- Other databases: creating mount points based on there installation guide.

As we do not control the NFS shares via the cluster in this setup, you should
add these file systems to _/etc/fstab_ to get the file systems mounted during
the next system boot.

////
review Lee means this looks like a 4 node cluster, adding additional title???
////
.File system layout including NFS shares
image::sles4sap_nw740_fs.svg[SVG]

We prepare the three servers for the distributed {sap} installation. Server 1
({myNode1}) will be used to install the ASCS {sap} instance. Server 2
({myNode2}) will be used to install the ERS {sap} instance. Server 3
({myNode3}) will be used to install the dialog {sap} instances and the database.

- Mounting the instance and database file systems at one specific node:

.{sapNW} 7.50 on x86_64 architecture with ASEDB
=============================================================

[subs="attributes"]
----
(ASCS   {myNode1}) # mount {myDevPartAscs} {myMpAscs}
(ERS    {myNode2}) # mount {myDevPartErs} /usr/sap/{mySid}/{myInstErs}
(DB     {myNode3}) # mount {myDevPartDb} /sybase/SSA/srsdata
(Dialog {myNode3}) # mount {myDevPartPas} /usr/sap/{mySid}/{myInstPas750}
(Dialog {myNode3}) # mount {myDevPartSec} /usr/sap/{mySid}/{myInstDSec}
----
=============================================================
////
.{sapNW} 7.50 on x86_64 architecture with HANA
=============================================================

[subs="attributes"]
----
(ASCS   {myNode1}) # mount {myDevPartAscs} {myMpAscs}
(ERS    {myNode2}) # mount {myDevPartErs} /usr/sap/{mySid}/{myInstErs}
(DB     {myNode3}) # mount {bsDevPartDbS} /hana/shared
(DB     {myNode3}) # mount {bsDevPartDbL} /hana/log
(DB     {myNode3}) # mount {bsDevPartDbD} /hana/data
(Dialog {myNode3}) # mount {myDevPartPas} /usr/sap/{mySid}/{myInstPas750}
(Dialog {myNode3}) # mount {myDevPartSec} /usr/sap/{mySid}/{myInstDSec}
----
=============================================================
////
- As a result the directory _/usr/sap/{mySid}/_ should now look like:

[subs="attributes"]
----
# ls -la /usr/sap/{mySid}/
total 0
drwxr-xr-x 1 {mySidLc}adm sapsys 70 28. Mar 17:26 ./
drwxr-xr-x 1 root   sapsys 58 28. Mar 16:49 ../
drwxr-xr-x 7 {mySidLc}adm sapsys 58 28. Mar 16:49 {myInstAscs}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mar 15:59 {myInstDSec}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mar 15:59 {myInstPas750}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mar 15:59 {myInstErs}/
drwxr-xr-x 5 {mySidLc}adm sapsys 87 28. Mar 17:21 SYS/
----

NOTE: The owner of the directory and files is changed during the {sap} installation. By default all
of them are owned by root.

== SAP Installation

The overall procedure to install the distributed SAP is:

- Installing the ASCS instance for the central services
- Installing the ERS to get a replicated enqueue scenario
- Preparing the ASCS and ERS installations for the cluster take-over
- Installing the Database
- Installing the primary application server instance (PAS)
- Installing additional application server instances (AAS)

The result will be a distributed {sap} installation as illustrated here:

.Distributed installation of the {sap} system
image::sles4sap_nw740_distInstall.svg[SVG]

=== Linux User and Group Number Scheme

Whenever asked by the SAP software provisioning manager (SWPM) which Linux User
IDs or Group IDs to use, refer to the following table which is, of course, only
an example.

[subs="attributes"]
----
Group sapinst      1000
Group sapsys       1001
Group sapadm       3000
Group sdba         3002

User  {mysapadm}       3000
User  sdb          3002
User  sqd{mySidLc}       3003
User  sapadm       3004
----
//TODO: is this correct?

=== Installing ASCS on {myNode1}

Temporarily we need to set the service IP address used later in the
cluster as local IP, because the installer wants to resolve or use it.
Make sure to use the right virtual host name for each installation step.
Take care for the ASCS file systems like {myDevPartAscs} and /sapcd/ (where the installation sources live) which might also need
to be mounted.

[subs="attributes"]
----
# ip a a {myVipAAscs}{myVipNM} dev eth0
# mount {myDevPartAscs} {myMpAscs}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNAscs}
----

* SWPM option depends on {sapNW} version and architecture
** Installing {sapNW} 7.5 -> SAP ASE -> Installation ->
Application Server ABAP -> High-Availability System -> ASCS Instance
* SID id {mySid}
* Use instance number {myAscsIno}
* Deselect using FQDN
* All passwords: use {mySapPwd}
* Double-check during the parameter review, if virtual name *{myVipNAscs}* is used

=== Installing ERS on {myNode2}

Temporarily we need to set the service IP address used later in the
cluster as local IP, because the installer wants to resolve or use it.
Make sure to use the right virtual host name for each installation step.

[subs="attributes"]
----
# ip a a {myVipAErs}{myVipNM} dev eth0
# mount {myDevPartErs} {myMpErs}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNErs}
----

* SWPM option depends on {sapNW} version and architecture
** Installing {sapNW} 7.5 -> SAP ASE -> Installation ->
Application Server ABAP -> High-Availability System -> Enqueue Replication
Server Instance
* Use instance number {myErsIno}
* Deselect using FQDN
* Double-check during the parameter review if virtual name *{myVipNErs}* is used
* If you get an error during the installation about permissions, change the
  ownership of the ERS directory

[subs="attributes"]
----
# chown -R {mysapadm}:sapsys /usr/sap/{mySid}/{myInstErs}
----

* If you get a prompt to manually stop/start the ASCS instance, log in at
{mynode1} as user {mysapadm} and call sapcontrol.

[subs="attributes"]
----
# sapcontrol -nr {myAscsIno} -function Stop    # to stop the ASCS
# sapcontrol -nr {myAscsIno} -function Start   # to start the ASCS
----

=== Poststeps for ASCS and ERS

==== Stopping ASCS and ERS

_On {myNode1}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myAscsIno} -function Stop
# sapcontrol -nr {myAscsIno} -function StopService
----

_On {myNode2}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myErsIno} -function Stop
# sapcontrol -nr {myErsIno} -function StopService
----

==== Maintaining _sapservices_

Ensure _/usr/sap/sapservices_ hold both entries (ASCS+ERS) on both cluster
nodes. This allows the {sapstartsrv} clients to start the service like
(do not execute this at this point in time).

_As user {mySapAdm}_

[subs="attributes"]
----
# sapcontrol -nr {myErsIno} -function StartService {mySid}
----
The _/usr/sap/sapservices_ looks like (typically one line per instance):

[subs="attributes"]
----
#!/bin/sh
LD_LIBRARY_PATH=/usr/sap/{mySid}/{myInstAscs}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/{mySid}/{myInstAscs}/exe/sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs} -D -u {mySapAdm}
LD_LIBRARY_PATH=/usr/sap/{mySid}/{myInstErs}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/{mySid}/{myInstErs}/exe/sapstartsrv pf=/usr/sap/{mySid}/{myInstErs}/profile/{mySid}_{myInstErs}_{myVipNErs} -D -u {mySapAdm}
----

==== Integrating the Cluster Framework Using {s4sClConnector3}

Install the package *{s4sClConnector3}* version 3.1.x from our
repositories:

[subs="attributes"]
----
# zypper in {s4sClConnector3}
----

NOTE: Be careful there are two packages available. The package {s4sClConnector}
continues to contain the old version 1.1.0 (SAP API 1). The package
{s4sClConnector3} contains the new version 3.1.x (SAP API 3).
The package {s4sClConnector3} with version 3.1.x implements the SUSE SAP API
version 3. New features like SAP Rolling Kernel Switch (RKS) and the migration of ASCS are
only supported with this new version.

For the ERS and ASCS instance edit the instance profiles
{mySid}_{myInstAscs}_{myVipNAscs} and {mySid}_{myInstErs}_{myVipNErs} in the
profile directory _/usr/sap/{mySid}/SYS/profile/_.

You need to tell the {sapStartSrv} to load the HA script connector library and
to use the {s4sClConnector3}.

[subs="attributes"]
----
service/halib = $(DIR_CT_RUN)/saphascriptco.so
service/halib_cluster_connector = /usr/bin/sap_suse_cluster_connector
----

Add the user {mySapAdm} to the unix user group haclient.

[subs="attributes"]
----
# usermod -aG haclient {mySapAdm}
----

==== Adapting {sap} Profiles to Match the {sapCert} Certification

For the ASCS, change the start command from _Restart_Programm_xx_ to
_Start_Programm_xx_ for the enqueue server (enserver). This change tells the
{sap} start framework *not* to self-restart the enqueue process. Such a restart
would lead in loss of the locks.

.File /usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs}

[subs="attributes"]
----
Start_Program_01 = local $(_EN) pf=$(_PF)
----

Optionally you could limit the number of restarts of services (in the case of
ASCS this limits the restart of the message server).

For the ERS change instance the start command from _Restart_Programm_xx_ to
_Start_Programm_xx_ for the enqueue replication server (enrepserver).

.File /usr/sap/{mySid}/SYS/profile/{mySid}_{myInstErs}_{myVipNErs}

[subs="attributes"]
----
Start_Program_00 = local $(_ER) pf=$(_PFL) NR=$(SCSID)
----

==== Starting ASCS and ERS

_On {myNode1}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myAscsIno} -function StartService {mySid}
# sapcontrol -nr {myAscsIno} -function Start
----

_On {myNode2}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myErsIno} -function StartService {mySid}
# sapcontrol -nr {myErsIno} -function Start
----

=== Installing DB on {myNode3} (Example MaxDB)

The MaxDB needs min.40 GB. We use {myDevPartDb} and mount the partition to
_/sapdb_.

A detailed description can be found link:https://documentation.suse.com/sbp/all/single-html/SAP_NW740_SLE12_SetupGuide/#_installing_db_on_hacert03_example_maxdb[here]

////
[subs="attributes"]
----
# ip a a {myVipADb}{myVipNM} dev eth0
# mount {myDevPartDb} {myMPDb}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNDb}
----

* We are installing SAP NetWeaver 7.40 SR2 -> MaxDB -> SAP-Systems ->
  Application Server ABAP -> High Availability System -> DB
* Profile directory /sapmnt/{mySid}/profile
* DB ID is {mySid}
* Volume Media Type *keep* File (not raw)
* Deselect using FQDN
* Double-check during the parameter review, if virtual name *{myVipNDb}* is used
////

=== Installing DB on {myNode3} (Example SAP HANA)

The HANA DB has very strict HW requirements. The storage sizing depends on many
indicators. Check the supported configurations at
https://www.sap.com/documents/2015/03/74cdb554-5a7c-0010-82c7-eda71af511fa.html[SAP HANA Hardware Directory]
and https://www.sap.com/documents/2015/03/74cdb554-5a7c-0010-82c7-eda71af511fa.html[SAP HANA TDI].

A detailed description can be found at https://documentation.suse.com/sbp/all/single-html/SAP_NW740_SLE12_SetupGuide/#_installing_db_on_hacert03_example_sap_hana[Example HANA DB].

////
[subs="attributes"]
----
# ip a a {myVipADb}{myVipNM} dev eth0
# mount {bsDevPartDbS} {bsMPDb}/shared
# mount {bsDevPartDbL} {bsMPDb}/log
# mount {bsDevPartDbD} {bsMPDb}/data
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNDb}
----

* We are installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> Database Instance
* Profile directory /sapmnt/{mySid}/profile
* Deselect using FQDN
* Database parameters:
  ** DBSID is _{bsSidDB}_
  ** Database Host is _{myVipNDb}_
  ** Instance Number is _{bsDBIno}_
* Database System ID:
  ** Instance Number is _{bsDBIno}_
  ** SAP Mount Directory is _{bsMPDb}/shared_
* Account parameters: change them in case of custom values needed
* Clean up: select *Yes*, remove operating system users from group'sapinst'....
* Double-check during the parameter review, if virtual name *{myVipNDb}* is
  used
////

=== Installing DB on {myNode3} (Example ASE DB)

TODO: content for that liegt bei SAP, back to original

=== Installing the Primary Application Server (PAS) on {myNode3}

////
[subs="attributes"]
----
# ip a a {myVipAPas}{myVipNM} dev eth0
# mount {myDevPartPas} {myMPPas74}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNPas}
----

or alternatively:
////

[subs="attributes"]
----
# ip a a {myVipAPas}{myVipNM} dev eth0
# mount {myDevPartPas} {myMPPas75}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNPas}
----

* SWPM option depends on {sapNW} version and architecture
** Installing {sapNW} 7.5 -> SAP ASE -> Installation ->
Application Server ABAP -> High-Availability System -> Primary Application Server Instance
(PAS)
* Use instance number {myPasIno}
* Deselect using FQDN
* For our hands-on setup use a default secure store key
* Do not install Diagnostic Agent
* No SLD
* Double-check during the parameter review if virtual name *{myVipNPas}* is used

=== Installing an Additional Application Server (AAS) on {myNode3}

[subs="attributes"]
----
# ip a a {myVipADSec}{myVipNM} dev eth0
# mount {myDevPartSec} {myMPSec}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNDSec}
----

* SWPM option depends on {sapNW} version and architecture
** Installing {sapNW} 7.5 -> SAP ASE -> Installation ->
Application Server ABAP -> High-Availability System -> Additional Application Server
Instance (AAS)
* Use instance number {myDSecIno}
* Deselect using FQDN
* Do not install Diagnostic Agent
* Double-check during the parameter review if virtual name *{myVipNDSec}* is used

== Implementing the Cluster

The main procedure to implement the cluster is as follows:

* Install the cluster software if not already done during the installation of
the operating system
* Configure the cluster communication framework corosync
* Configure the cluster resource manager
* Configure the cluster resources
* Tune the cluster timing in special for the SBD.

///////////////////////////////
TODO: Do we really need to stop, unconfigure and unmount?
Maybe we find a way to configure the resources that the cluster just
accepts the already started resource groups - lets see ;-)

NOTE: Before we continue to set up the cluster, we first stop all SAP instances, remove
the (manual added) IP addresses on the cluster nodes and unmount the file systems
which will be controlled by the cluster later.
///////////////////////////////

NOTE: The SBD device/partition need to be created in beforehand. In this setup
guide we do not use the SBD device.

.Tasks

. Setup NTP (best with yast2) and enable it

. Install pattern ha_sles on both cluster nodes

[subs="attributes"]
----
# zypper in -t pattern ha_sles
----

=== Configuring the Cluster Base

.Tasks

- Install and configure the cluster stack at first machine

You can use either YaST to configure the cluster base or the interactive
command line tool ha-cluster-init. The following script can be used for
automated setups.

[subs="attributes"]
----
# modprobe softdog
# echo "softdog" > /etc/modules-load.d/softdog.conf
# systemctl enable sbd
# ha-cluster-init -y -i eth0 -u -s {myDevPartSbd}
----

Keep in mind that a hardware watchdog is preferred instead of the softdog method.

- Join the second node

Find below some preparation steps on the second node.

[subs="attributes"]
----
# modprobe softdog
# echo "softdog" > /etc/modules-load.d/softdog.conf
# systemctl enable sbd
# rsync {myIpNode1}:/etc/sysconfig/sbd /etc/sysconfig
----

You can use either YaST to configure the cluster base or the interactive
command line tool ha-cluster-join. The following script can be used for
automated setups.

[subs="attributes"]
----
# ha-cluster-join -y -c {myIPNode1} -i {myHaNetIf}
----

- The _crm_mon -1r_ output should look like this:

[subs="attributes"]
----
Last updated: Thu Nov 21 14:25:53 2019		Last change: Thu Nov 21 14:23:21 2019 by {mySidLc}adm via crm_resource on {myNode1}
Stack: corosync
Current DC: {myNode1} (version 1.1.19-20181105.ccd6b5b10) - partition with quorum
2 nodes and 1 resource configured

Online: [ {myNode1} {myNode2} ]

stonith-sbd	(stonith:external/sbd):	Started {myNode1}
----

- After both nodes are listed in the overview, verify the property setting of the basic cluster configuration.
Very important here is the setting: *record-pending=true*.

[subs="attributes"]
----
# crm configure show
...
property cib-bootstrap-options: \
        have-watchdog=true \
        dc-version=1.1.19-20181105.ccd6b5b10 \
        cluster-infrastructure=corosync \
        cluster-name=hacluster \
        stonith-enabled=true \
        last-lrm-refresh=1494346532
rsc_defaults rsc-options: \
        resource-stickiness=1 \
        migration-threshold=3
op_defaults op-options: \
        timeout=600 \
        record-pending=true

----

=== Configuring Cluster Resources

We need a changed SAPInstance resource agent for {sapNw} to *not* use
the master-slave construct anymore and to move to a more cluster-like construct to
start and stop the ASCS and the ERS itself and *not* only the complete
master-slave.

For this there is a new functionality for the ASCS needed to follow the ERS.
The ASCS needs to mount the shared memory table of the ERS to avoid the loss of
locks.

.Resources and constraints
image::sles4sap_nw740_resources.svg[SVG]

The implementation is done using the new flag "runs_ers_$SID" within
the RA, enabled with help of the resource parameter "IS_ERS=TRUE".

Another benefit of this concept is that we can now work with local (mountable)
file systems instead of a shared (NFS) file system for the {sap} instance
directories.

==== Preparing the Cluster for Adding the Resources

To avoid that the cluster starts partially defined resources, we set the cluster
to the maintenance mode. This deactivates all monitor actions.

_As user root_

[subs="attributes"]
----
# crm configure property maintenance-mode="true"
----

==== Configuring the Resources for the ASCS

First we configure the resources for the file system, IP address and the {sap}
instance. Of course you need to adapt the parameters to your environment.

.ASCS primitive
================================================
[subs="attributes"]
----
primitive rsc_fs_{mySID}_{myInstAscs} Filesystem \
  params device="{myDevPartAscs}" directory="/usr/sap/{mySID}/{myInstAscs}" \
     fstype=xfs \
  op start timeout=60s interval=0 \
  op stop timeout=60s interval=0 \
  op monitor interval=20s timeout=40s
primitive rsc_ip_{mySID}_{myInstAscs} IPaddr2 \
  params ip={myVipAAscs} \
  op monitor interval=10s timeout=20s
primitive rsc_sap_{mySID}_{myInstAscs} SAPInstance \
  operations $id=rsc_sap_{mySID}_{myInstAscs}-operations \
  op monitor interval=11 timeout=60 on-fail=restart \
  params InstanceName={mySID}_{myInstAscs}_{myVipNAscs} \
     START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstAscs}_{myVipNAscs}" \
     AUTOMATIC_RECOVER=false \
  meta resource-stickiness=5000 failure-timeout=60 \
     migration-threshold=1 priority=10
----
================================================

.ASCS group
================================================
[subs="attributes"]
----
group grp_{mySID}_{myInstAscs} \
  rsc_ip_{mySID}_{myInstAscs} rsc_fs_{mySID}_{myInstAscs} rsc_sap_{mySID}_{myInstAscs} \
     meta resource-stickiness=3000
----
================================================

Create a txt file (like crm_ascs.txt) with your preferred text editor, enter
both examples (primitives and group) to that file and load the configuration to
the cluster manager configuration.

_As user root_

[subs="attributes"]
----
# crm configure load update crm_ascs.txt
----

==== Configuring the Resources for the ERS

Second, we configure the resources for the file system, IP address and the {sap}
instance. Of course you need to adapt the parameters to your environment.

The specific parameter _IS_ERS=true_ should only be set for the ERS instance.

.ERS primitive
================================================
[subs="attributes"]
----
primitive rsc_fs_{mySID}_{myInstErs} Filesystem \
  params device="{myDevPartErs}" directory="/usr/sap/{mySID}/{myInstErs}" fstype=xfs \
  op start timeout=60s interval=0 \
  op stop timeout=60s interval=0 \
  op monitor interval=20s timeout=40s
primitive rsc_ip_{mySID}_{myInstErs} IPaddr2 \
  params ip={myVipAErs} \
  op monitor interval=10s timeout=20s
primitive rsc_sap_{mySID}_{myInstErs} SAPInstance \
  operations $id=rsc_sap_{mySID}_{myInstErs}-operations \
  op monitor interval=11 timeout=60 on-fail=restart \
  params InstanceName={mySID}_{myInstErs}_{myVipNErs} \
     START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstErs}_{myVipNErs}" \
     AUTOMATIC_RECOVER=false IS_ERS=true \
  meta priority=1000
----
================================================

.ERS group
================================================
[subs="attributes"]
----
group grp_{mySID}_{myInstErs} \
  rsc_ip_{mySID}_{myInstErs} rsc_fs_{mySID}_{myInstErs} rsc_sap_{mySID}_{myInstErs}
----
================================================

Create a txt file (like crm_ers.txt) with your preferred text editor, enter
both examples (primitives and group) to that file and load the configuration to
the cluster manager configuration.

_As user root_

[subs="attributes"]
----
# crm configure load update crm_ers.txt
----

==== Configuring the Colocation Constraints Between ASCS and ERS

The constraints between the ASCS and ERS instance are needed to define that the
ASCS instance starts exactly on the cluster node running the ERS
instance after a failure (loc_sap_{mysid}_fail-over_to_ers). This constraint is
needed to ensure that the locks are not lost after an ASCS instance (or node)
failure.

If the ASCS instance has been started by the cluster the ERS instance should
be moved to an "other" cluster node (col_sap_{mysid}_no_both). This constraint
is needed to ensure that the ERS will synchronize the locks again and the cluster is
ready for an additional take-over.

.Location constraint
================================================
[subs="attributes"]
----
colocation col_sap_{mysid}_no_both -5000: grp_{mySID}_{myInstErs} grp_{mySID}_{myInstAscs}
location loc_sap_{mysid}_fail-over_to_ers rsc_sap_{mySID}_{myInstAscs} \
         rule 2000: runs_ers_{mySID} eq 1
order ord_sap_{mysid}_first_start_ascs Optional: rsc_sap_{mySID}_{myInstAscs}:start \
      rsc_sap_{mySID}_{myInstErs}:stop symmetrical=false
----
================================================

Create a txt file (like crm_col.txt) with your preferred text editor, enter
all three constraints to that file and load the configuration to the
cluster manager configuration.

_As user root_

[subs="attributes"]
----
# crm configure load update crm_col.txt
----

==== Activating the Cluster

Now the last step is to end the cluster maintenance mode and to allow the
cluster to detect already running resources.

_As user root_

[subs="attributes"]
----
# crm configure property maintenance-mode="false"
----

////
############## REMOVING THIS FROM THE OUTPUT FOR NOW ###########

=== Installing SAP Licenses

Most likely you have your own established method to maintain your {sap} licenses.
This section is only a reminder. You should have installed two license keys as
we need to fail-over the ASCS {sap} instance.

- Get the HWKEY of both cluster nodes
- Get the license from the SAP launchpad
- Install the two licenses per HWKEY using transaction SLICENSE

###################
////

== Administration

=== Dos and Don'ts

==== Never Stop the ASCS Instance

For normal operation *do not stop* the ASCS {sap} instance with any tool such
as cluster tools or {sap} tools. The stop of the ASCS instance might lead to a loss of enqueue
locks. Because following the new {sapCert} certification the cluster must allow local restarts
of the ASCS. This feature is needed to allow rolling kernel switch (RKS) updates without
reconfiguring the cluster.

WARNING: Stopping the ASCS instance might lead into the loss of {sap} enqueue
  locks during the start of the ASCS on the same node.

==== How to Move ASCS

To *move* the ASCS {sap} instance you should use the {sap} tools such as
the {sap} management console. This will trigger {sapStartSrv} to use the
{s4sClConnector3} to move the ASCS instance. As user _{mysapadm}_ you might call
the following command to move-away the ASCS. The move-away will always
move the ASCS to the ERS side which will keep the {sap} enqueue locks.

_As {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr {myAscsIno} -function HAfailoverToNode ""
----

==== Never Block Resources

With {sapCert} it is *not longer allowed to block resources* from beeing
  controlled manually. This using the variable _BLOCK_RESOURCES_ in
  _/etc/sysconfig/sap_suse_cluster_connector_ is not allowed anymore.

==== Always Use Unique Instance Numbers

Currently all {sap} *instance numbers controlled by the cluster must be unique*.
  If you need to have multiple dialog instances such as D00 running on different
  systems they should be not controlled by the cluster.

==== How to Set Cluster in Maintenance Mode

The procedure to set the cluster into maintenance mode can be done as _root_ or _sidadm_.

_As user root_

[subs="attributes"]
----
# crm configure property maintenance-mode="true"
----

_As user {mysapadm} (the full path is needed)_

[subs="attributes"]
----
# /usr/sbin/crm configure property maintenance-mode="true"
----

==== Procedure to End the Cluster Maintenance

_As user root_

[subs="attributes"]
----
# crm configure property maintenance-mode="false"
----

==== Cleaning Up Resources

How to *clean up resource failures*? Failures of the ASCS will be automatically
  deleted to allow a failback after the configured period of time. For all other
  resources you can clean up the status including the failures:

_As user root_

[subs="attributes"]
----
# crm resource refresh RESOURCE-NAME
----

WARNING: You should not clean up the complete group of the ASCS resource as this
   might lead into an unwanted cluster action to take-over the complete group to
   the node where ERS instance is running.

=== Testing the Cluster

We strongly recommend that you at least process the following tests before you
plan going into production with your cluster:

==== Checking Product Names with HAGetFailoverConfig

Check if the name of the SUSE cluster solution is shown in the output of
  sapcontrol or {sap} management console. This test checks the status of the
  {sapNW} cluster integration.

_As user {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr {myAscsIno} -function HAGetFailoverConfig
----

==== Starting SAP Checks Using HACheckConfig and HAGetFailoverConfig

Check if the HA configuration tests are showing no errors.

_As user {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr {myAscsIno} -function HACheckConfig
# sapcontrol -nr {myAscsIno} -function HAGetFailoverConfig
----

==== Manually Moving ASCS

Check if manually moving the ASCS using HA tools works properly.

_As user root_

[subs="attributes"]
----
# crm resource move rsc_sap_{mySid}_{myInstAscs} force
## wait until the ASCS is been moved to the ERS host
# crm resource clear rsc_sap_{mySid}_{myInstAscs}
----

==== Migrating ASCS Using HAfailoverToNode

Check if moving the ASCS instance using {sap} tools like {sapCtrl} does work properly

_As user {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr {myAscsIno} -function HAfailoverToNode ""
----

==== Testing ASCS Migration After Failure

Check if the ASCS instance moves correctly after a node failure.

_As user root_

[subs="attributes"]
----
## on the ASCS host
# echo b >/proc/sysrq-trigger
----

==== Inplacing Restart of ASCS Using Stop and Start

Check if the in-place re-start of the {sap} resources have been processed
  correctly. The {sap} instance should not failover to an other node, it
  must start on the same node where it has been stopped.

WARNING: This test will force the SAP system to *lose* the enqueue locks.
   *This test should not be processed during production.*

_As user {mysapadm}_

[subs="attributes"]
----
## example for ASCS
# sapcontrol -nr {myAscsIno} -function Stop
## wait until the ASCS is completely down
# sapcontrol -nr {myAscsIno} -function Start
----

==== Automated Restart of the ASCS Instance (Simulating Rolling Kernel Switch)

The next test should proof that the cluster solution did nor interact neither try to restart the ASCS instance
during a maintenance procedure. In addition, it should verify that no locks are lost during the restart of
an ASCS instance during an RKS procedure. The cluster solution should recognize that the restart of
the ASCS instance was expected. No failure or error should be reported or counted.

Optionally, you can set locks and verify that they still exist after the maintenance procedure. There are multiple
ways to do that. One example test can be performed as follows:

. Log in to your SAP system and open the transaction SU01.
. Create a new user. Do not finish the transaction to see the locks.
. With the SAP MC / MMC, check if there are locks available.
. Open the ASCS instance entry and go to _Enqueue Locks_.
. With the transaction SM12, you can also see the locks.

Do this test multiple times in a short time frame. The restart of the ASCS instance in the example below happens five times.

As user _{mysapadm}_, create and execute the following script:
[subs="attributes"]
----
$ cat ascs_restart.sh
#!/bin/bash
for lo in 1 2 3 4 5; do
  echo LOOP "$lo - Restart ASCS{myAscsIno}"
  sapcontrol -host sap{mySidLc}as -nr {myAscsIno} -function StopWait 120 1
  sleep 1
  sapcontrol -host sap{mySidLc}as -nr {myAscsIno} -function StartWait 120 1
  sleep 1
done
----
[subs="attributes"]
----
$ bash ascs_restart.sh
----

==== Rolling Kernel Switch Procedure

The rolling kernel switch (RKS) is an automated procedure that enables the kernel in an ABAP system
to be exchanged without any system downtime. During an RKS, all instances of the system, and
generally all SAP start services (sapstartsrv), are restarted.

. Check in SAP note 953653 whether the new kernel patch is RKS compatible to your currently running kernel.
. Check SAP note 2077934 - Rolling kernel switch in HA environments.
. Download the new kernel from the SAP service market place.
. Make a backup of your current central kernel directory.
. Extract the new kernel archive to the central kernel directory.
. Start the RKS via SAP MMC, system overview (transaction SM51) or via command line.
. Monitor and check the version of your SAP instances with the SAP MC / MMC or with *sapcontrol*.

As user _{mysapadm}_, type the following commands:

[subs="specialchars,attributes"]
----
## sapcontrol [-user <sidadm psw>] -host <host> -nr <INSTANCE_NR> -function UpdateSystem 120 300 1
# sapcontrol -user {mySapAdm} {mySapPwd} -host {myVipNAscs} -nr {myAscsIno} -function UpdateSystem 120 300 1
# sapcontrol -nr {myAscsIno} -function GetSystemUpdateList -host {myVipNAscs} \
  -user {mySapAdm} {mySapPwd}
# sapcontrol -nr {myAscsIno} -function GetVersionInfo -host {myVipNAscs} \
  -user {mySapAdm} {mySapPwd}
# sapcontrol -nr {myErsIno} -function GetVersionInfo -host {myVipNErs} \
  -user {mySapAdm} {mySapPwd}
# sapcontrol -nr {myPasIno} -function GetVersionInfo -host {myVipNPas} \
  -user {mySapAdm} {mySapPwd}
# sapcontrol -nr {myDSecIno} -function GetVersionInfo -host {myVipNDSec} \
  -user {mySapAdm} {mySapPwd}
----

==== Additionally Recommended Tests

* Check the recoverable and non-recoverable outage of the message server process

* Check the non-recoverable outage of the {sap} enqueue server process

* Check the outage of the {sapERS}

* Check the outage and restart of {sapStartSrv}

* Check the simulation of an upgrade

* Check the simulation of cluster resource failures

////
=== Pro's and Con's for Odd and Even Numbers of Cluster Nodes

There are certain use cases and infrastructure requirements which end up in different installation setup's.
We will cover some advantages and disadvantages of special configuration below:

* The two node cluster and two locations
    - Advantage: symmetric spread of all nodes over all locations
    - Disadvantage: no diskless SBD feature allowed for all two node clusters
* The two node cluster and more than two locations
    - Advantage: SBD device can be provided from there (must be HA himself)
    - Advantage: cluster could operate with three SBD devices from different locations
    - Disadvantage: no diskless SBD feature allowed for all two node clusters
* The three node cluster and two locations
    - Advantage: less complex infrastructure
    - Advantage: diskless SBD feature is allowed
    - Disadvantage: "pre selected" location (two node + one node)
* The three node cluster and three locations
    - Advantage: symmetric spread of all nodes over all locations
    - Advantage: diskless SBD feature is allowed
    - Disadvantage: higher planing effort and complexity for infrastructure planning
////

== Additional Implementation Scenarios

=== Adaptive Server Enterprise Replication fail-over Automation Integration

==== FM Integration with SUSE Linux Enterprise High Availability Extension Cluster

Standard SAP on AWS for an HA setup is Multi-AZ deployment with ASCS, Primary DB running in
one AZ and their counterpart ERS and Secondary DB running in the second AZ of the same region.
The Primary Application Server & Additional Application servers based on the load can be distributed
in both AZ’s as well to provide resiliency.
Considering a scenario where SAP NetWeaver or Business Suite system is running on SAP Sybase ASE.
The completely automated HA for the ABAP Stack (ASCS) is provided by the SUSE Linux Enterprise High Availability Extension Cluster. For
the Sybase ASE DB the HA feature is provided with the Always On configuration and the fail-over
orchestration is done by the Fault Manager (FM) utility which traditionally was installed on a third host
(other than the Primary & Secondary DB). In an SAP world the FM utility comes along with an SAP DB
dependent Kernel and gets installed in the ASCS Work directory _/usr/sap/<SID>/ASCS<instnr>/exe/_. The
fail-over of the ASCS instance along with the associated directories (provided they are installed on a
shared file system using either Amazon EFS or NFS) is taken care by the SUSE Linux Enterprise High Availability Extension Cluster.

==== Sybase ASE Always On

SAP Sybase ASE comes with an Always On feature which provides native HA & DR capability. The
always-on option is a high availability and disaster recovery (HADR) system that consists of two
SAP ASE servers: One is designated as the primary server, on which all transaction processing takes place. The
other acts as a warm standby (called "standby server" in DR mode, and as a "companion" in
HA mode) for the primary server, and contains copies of designated databases from the primary server.
The fail-over orchestration is carried out by ASE provided utility called Fault Manager. The Fault
Manager monitors the various components of the HADR environment – Replication Management Agent
(RMA), ASE, Replication Server, applications, databases, and the operating system. Its primary job is
to ensure the high-availability (zero data loss during fail-over) of the ASE cluster by initiating automatic
fail-over with minimal manual intervention. In an SAP Stack, the fault manager utility (sybdbfm) comes
as part of the DB (Sybase ASE) dependent SAP Kernel.
Refer to the SAP Standard ASE HA-DR guide (https://help.sap.com/viewer/efe56ad3cad0467d837c8ff1ac6ba75c/16.0.3.6/en-US/a6645e28bc2b1014b54b8815a64b87ba.html)
for setting up the Sybase ASE DB in HA mode.

IMPORTANT: In the following section we use sometimes examples and sometimes general examples. In the general are terms like <SID>; <instance nr>. They must be adapted to your environment. As an example, _su - <sid>adm_ means _su - {mySidLc}adm_ or in capital letters _cd /usr/sap/<SID>/ASCS<instance nr>/work_ means _cd /usr/sap/{mySid}/ASCS00/work_

==== Database Host Preparation

This guide does not duplicate the official HADR documentation. The following procedure describes the
key points which you need to take care of.

.Installation 32-bit Environment

[subs="specialchars,attributes"]
----
# zypper install glibc-32bit libgcc_s1-32bit
----

For the example this software stack is used:

* SL TOOLSET 1.0 -- SWPM -> 1.0 SP25 for NW higher than 7.0x
* saphostagent -> 7.21 patch 41
* SAP Kernel -> 7.53 PL421
* SAP Installation Export ->  (51051806_1)
* Sybase RDBMS->  ASE 16.0.03.06 RDBMS (51053561_1)

NOTE: Very useful is that short table of installation information which helps to be prepared for the next steps.
SAP Adaptive Server Enterprise - Installation Worksheet
https://help.sap.com/viewer/efe56ad3cad0467d837c8ff1ac6ba75c/16.0.3.6/en-US/3fe35550f3814b2bb411d5494976e25a.html

IMPORTANT: The Fault Manger is enhanced to work in this setup. The minimal version's which support this scenario are
* SAP Kernel 749 PL632
* SAP Kernel 753 PL421

==== Database Installation for Replication Scenario

The installation can be done with the SWPM which is provided by SAP.

.Installing the primary database with SWPM:
* SWPM option depends on {sapNW} version and architecture
** Software Provisioning Manager 1.0 SP 25 -> SAP NetWeaver AS for ABAP 7.52 -> SAP ASE -> Installation -> Application Server ABAP -> High-Availability System -> Database Instance

The following information is requested from the wizard:

* Master Password <secure>
* SAP System Code Page: Unicode (default)
* Uncheck: -> Set FQDN for SAP system
* Sybase database Administrator UID: 2003
* In our test setup we uncheck -> Use separate devices for sybmgmtdb database

After the basis installation is finished the primary database must be prepared for the replication. First
the user *sa* must be unlocked.

[subs="specialchars,attributes"]
----
# su - syb<sid>
# isql -Usapsso -P <secure password> -S<SID> -X -w1900
# 1> go
# 1> exec sp_locklogin sa, 'unlock'
# 2> go
# Account unlocked.
# (return status = 0)
# 1> quit
----

In the next step, install the SRS software with a response file and enter the following command as user _syb<sid>_:

[subs="specialchars,attributes"]
----
# /sapcd/ase-16.0.03.06/BD_SYBASE_ASE_16.0.03.06_RDBMS_for_BS_/SYBASE_LINUX_X86_64/setup.bin -f /sybase/{mySid}/srs-setup.txt -i silent
----

Activate HADR on primary node with a response file and enter the following command as user _syb<sid>_:

[subs="specialchars,attributes"]
----
# setuphadr /sybase/{mySid}/{mySid}_primary_lin.rs.txt
----

NOTE: If the installation stops with an error message as diplayed here, perform the steps explained below:

[subs="specialchars,attributes"]
----
Clean up environment.
Environment cleaned up.
Error: Fail to connect to "PRIM" site SAP ASE at "<hostname>:4901".
----

Check if the host name and port number are correct and the database server is up and running. If everything is correct and network connection should be available, it might help to modify the _interface_ file. Try to add a new line in the _/sybase/<SID>/interfaces_ file for the <SID> section with the IP address of the corresponding host name.

[subs="specialchars,attributes"]
----
# vi /sybase/<SID>/interfaces
...
	master tcp ether <hostname> 4901
	master tcp ether 172.17.1.21 4901
...
----

Create a secure store key entry for the database:

[subs="specialchars,attributes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -user sapadm <secure password> -function LiveDatabaseUpdate -dbname <SID> -dbtype syb -dbuser DR_admin -dbpass <Secure password> -updatemethod Execute -updateoption TASK=SET_USER_PASSWORD -updateoption USER=DR_ADMIN
----

.Installing the companion database with SWPM:
* SWPM option depends on {sapNW} version and architecture
** Software Provisioning Manager 1.0 SP 25 -> SAP NetWeaver AS for ABAP 7.52 -> SAP ASE -> Database Replication -> Setup of Replication Environment

The following information is requested from the wizard:

* Replication System Parameters -> SID, Master Password, check Set up a secondary database instance
* Primary Database server -> host name or virt. name
* Primary Database server port -> default is 4901, depends on the setup of your primary server

After the basis installation is finished the companion database must be prepared for the replication. First
the user *sa* must be unlocked.

[subs="specialchars,attributes"]
----
# su - syb<sid>
# isql -Usapsso -P <secure password> -S<SID> -X -w1900
# 1> go
# 1> exec sp_locklogin sa, 'unlock'
# 2> go
# Account unlocked.
# (return status = 0)
# 1> quit
----

Next step installing the SRS software with a response file on the companion site and enter the following command as user syb<sid>:

[subs="specialchars,attributes"]
----
# /sapcd/ase-16.0.03.06/BD_SYBASE_ASE_16.0.03.06_RDBMS_for_BS_/SYBASE_LINUX_X86_64/setup.bin -f /sybase/{mySid}/srs-setup.txt -i silent
----

Activate HADR on companion node with a response file and enter the following command as user syb<sid>:

[subs="specialchars,attributes"]
----
# setuphadr /sybase/{mySid}/{mySid}_companion_lin.rs.txt
----

NOTE: In certain circumstances the installation is not successful. It could help to set up the primary
system again and install the companion afterward.

NOTE: If the system is reinstalled and the companion system reports *Missing read/write
permissions* for this directory _/tmp/.SQLAnywhere_, check the permission on both node. In case
the ownership must be changed run the setup again on both nodes. Start with the *Primary*.

Creating a secure store key entry for the database:

[subs="specialchars,attributes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -user sapadm <secure password> -function LiveDatabaseUpdate -dbname <SID> -dbtype syb -dbuser DR_admin -dbpass <Secure password> -updatemethod Execute -updateoption TASK=SET_USER_PASSWORD -updateoption USER=DR_ADMIN
----

==== Fault Manager Installation

NOTE: Please remember in this scenario the FM will be integrated into a cluster who takes already care of the ASCS and ERS of a SAP system.
The goal is to make the FM high available itself and reuse existing resources.

//TODO: Explain the two installation option, for business suite and normal
//TODO: requirement overview for standalone FM, storage, IP, hostname, rewrite text below to fit both installations

The Fault Manager is configured on the ASCS host. The benefit from this setup is that the sybdbfm service
can be monitored and tracked with the existing pacemaker for the ASCS / ERS replication.

Option one:

* installation of FM service as part of ASCS (Business Suite???)

Option two:

* standalone installation of FM (non Business Suite???)
** to make this type of installation ready for a pacemaker implementation, additional requirements needs to be full filled
** a filesystem ~ 2GB which can be moved between all cluster nodes
** virtual hostname for FM instance
** a free instance number of the SAP system which is already implemented in the cluster
** a virtual IP address which can be moved between all cluster nodes

NOTE: Depending on the integration of FM later into the pacemaker cluster, additional storage and IP resources are required.
Please check <<Cluster-Integration-of-Fault-Manager>> before you start the installation.

.Fault Manager Installation as part of the ASCS instance
====
[subs="specialchars,attributes"]
----
# su - <sid>adm
# cd /usr/sap/<SID>/ASCS<instance number>/exe/
# sybdbfm install
----

This is an example of the installation process:
[subs="specialchars,attributes,quotes,verbatim"]
----
replication manager agent user DR_admin and password set in Secure Store.
Keep existing values (yes/no)? (yes)
SAPHostAgent connect user sapadm and password set in Secure Store.
Keep existing values (yes/no)? (yes)
Enter value for primary database host: ({myVipNDbA1})
{myVipNDbA1}
Enter value for primary database name: ({mySid})
Enter value for primary database port: (4901)
Enter value for primary site name: (FRA1)
Enter value for primary database heart beat port: (13777)
Enter value for standby database host: ({myVipNDbA2})
{myVipNDbA1}
Enter value for standby database name: ({mySid})
Enter value for standby database port: (4901)
Enter value for standby site name : (FRA2)
Enter value for standby database heart beat port: (13787)
Enter value for fault manager host: ({myVipNFM})
Enter value for heart beat to heart beat port: (13797)
Enter value for support for floating database ip: (no)
Enter value for use SAP ASE Cockpit if it is installed and running: (no)
----
====

Update the values as per your environment for the Primary DB & companion DB host name, SID &
Site Name. Make sure to use the virtual host name for the ASCS host. When the Fault Manager is installed,
profile for it will be created in the _/sapmnt/<SID>/profile_ by the name _SYBHA.PFL_ and will have the
configuration details.
Restart the ASCS Instance which will also start the Fault Manager that has been added to the start profile as below:

NOTE: In case of a re-installation it might be better to overwrite the existing user name and password in the secure store for the _sapadm_ and _DR_admin_ if the old values are not 100% known.

.Fault Manager Installation as standalone service
======================================
The following preparation steps are needed to make the FM service high available and flexible as possible.

- creating new mount point on *all* nodes where FM should run later
- mounting shared filesystem (iSCSI, FC-LUN, NFS)
- manual adding vIP address for FM instance
- adapting the _/etc/hosts_ with vIP and hostname of FM on *all* nodes where FM should run later

For the example below we used this values:
SID: {mySid} (the same SID of the SAP system where the DB is connected too simplifies the integration)
instance number: {aseIno} (new)
instance name: {aseInstN} (new)
virtual IP: {myVipAFM} (new), (overlay IP address)
virtual hostname: {myVipNFM} (new)
storage: {myDevPartFM} (new), (NFS4 cloud storage)

[subs="specialchars,attributes"]
----
# mkdir -p /usr/sap/{mySid}/{aseInst}
# ip a a {myVipAFM} dev eth0
# vi /etc/hosts
# mount {myDevPartFM} /usr/sap/{mySid}/{aseInst}
# chown {mySapAdm}.sapsys /usr/sap/{mySid}/{aseInst}
# su - {mySapAdm}
# cd /usr/sap/{mySid}/{aseInst}
## extract the FM installation media using SAPCAR
# SAPCAR -xvf SYBCTRL_611-80002616.SAR
SAPCAR: processing archive SYBCTRL_611-80002616.SAR (version 2.01)
x patches.mf
x startdb
x stopdb
x sybctrl
x sybdbfm
x sybfm
----

As _{mysapadm}_ install the Fault Manager
[subs="specialchars,attributes"]
----
# su - <sid>adm
# cd /usr/sap/{mySid}/{aseInst}
# sybdbfm install
----
//TODO: what is missing???
======================================
.ASCS profile after FM installation as integrated service
======================================
[subs="specialchars,attributes"]
----
# cat /sapmnt/<SID>/profile/<SID>_ASCS<instance number>_<virt. ASCS hostname>
....
#-----------------------------------------------------------------------
# copy sybdbfm and dependent
#-----------------------------------------------------------------------
_CP_SYBDBFM_ARG1 = list:$(DIR_CT_RUN)/instancedb.lst
Execute_06 = immediate $(DIR_CT_RUN)/sapcpe$(FT_EXE) pf=$(_PF) $(_CP_SYBDBFM_ARG1)
_CP_SYBDBFM_ARG2 = list:$(DIR_GLOBAL)/syb/linuxx86_64/cpe_sybodbc.lst
_CP_SYBDBFM_ARG3 = source:$(DIR_GLOBAL)/syb/linuxx86_64/sybodbc
Execute_07 = immediate $(DIR_CT_RUN)/sapcpe$(FT_EXE) pf=$(_PF) $(_CP_SYBDBFM_ARG2) $(_CP_SYBDBFM_ARG3)
#-----------------------------------------------------------------------
# Start sybha
#-----------------------------------------------------------------------
_SYBHAD = sybdbfm.sap$(SAPSYSTEMNAME)_$(INSTANCE_NAME)
_SYBHA_PF = $(DIR_PROFILE)/SYBHA.PFL
Execute_08 = local rm -f $(_SYBHAD)
Execute_09 = local ln -s -f $(DIR_EXECUTABLE)/sybdbfm$(FT_EXE) $(_SYBHAD)
Restart_Program_02 = local $(_SYBHAD) hadm pf=$(_SYBHA_PF)
#-----------------------------------------------------------------------
....
----
======================================

A few parameters that need to be updated in the _SYBHA.PFL_ to make the fail-over working.

* Option 1: ASCS integration
* Option 2: independent integration

.For option 1 the _SYBHA.PFL_ file in case of ASCS integration.
======================================
[subs="specialchars,attributes,verbatim,quotes"]
----
ha/syb/support_cluster = 1
ha/syb/fail-over_if_unresponsive = 1
ha/syb/allow_restart_companion = 1
ha/syb/set_standby_available_after_fail-over = 1
ha/syb/chk_restart_repserver = 1
ha/syb/cluster_fmhost1 = **Hostname for Node 1 of the ASCS HA Setup**
ha/syb/cluster_fmhost2 = **Hostname for Node 2 of the ASCS HA Setup**
ha/syb/use_boot_file_always = 1
ha/syb/dbfmhost = **virtual hostname of ASCS instance**
----
======================================

.For option 2 the _SYBHA.PFL_ file in case of independent integration.
======================================
[subs="specialchars,attributes,verbatim,quotes"]
----
ha/syb/support_cluster = 1
ha/syb/fail-over_if_unresponsive = 1
ha/syb/allow_restart_companion = 1
ha/syb/set_standby_available_after_fail-over = 1
ha/syb/chk_restart_repserver = 1
ha/syb/cluster_fmhost1 = **Hostname for Node 1 of the HA Setup**
ha/syb/cluster_fmhost2 = **Hostname for Node 2 of the HA Setup**
ha/syb/use_boot_file_always = 1
ha/syb/dbfmhost = **virtual hostname of FM instance**
----
======================================

Details of all the FM parameters can be found in the *SAP ASE HA DR User Guide*. Those highlighted
in bold are of interest for the setup. Since the FM is installed with the ASCS which can
fail-over from Node 1 to Node 2, the parameters _ha/syb/cluster_fmhost1_ and
_ha/syb/cluster_fmhost2_ provide the physical host names of both nodes where the FM can potentially run.

//TODO: restart of ASCS required???? direction of commands???

._As user {mysapadm}_ check if the _sybdbfm_ process is shown.
======================================
The example below show the FM integration as part of the ASCS instance.

[subs="specialchars,attributes,quotes,verbatim"]
----
{mysapadm}> sapcontrol -nr {myAscsIno} -function GetProcessList

23.02.2020 22:11:52
GetProcessList
OK
name, description, dispstatus, textstatus, starttime, elapsedtime, pid
msg_server, MessageServer, GREEN, Running, 2020 04 22 18:28:31, 27:43:21, 17731
enserver, EnqueueServer, GREEN, Running, 2020 04 22 18:28:31, 27:43:21, 17732
sybdbfm, , GREEN, Running, 2020 04 22 18:28:31, 27:43:21, 17733
----
======================================

In a scenario where the complete Availability Zone (AZ1) goes down and the ASCS and Primary database are running there, the DB fail-over is not triggered
until the ASCS fail-over is complete and the FM is up and running in the 2nd Availability Zone (AZ2). The FM then needs to read the boot file
to get the prior state of the ASE DB. This is mandatory to ensure that FM can trigger the fail-over correctly. The parameter
_ha/syb/use_boot_file_always=1_ makes sure that the FM always reads from the boot file which is part of the work
directory (the same for ASCS and FM) and fail-over along with FM.

.FM status check and DB replication information
======================================
The status of the FM can be checked as below. Navigate to the ASCS work directory and then run
_sybdbfm.sap.<SID>_ASCS<instance number> status_ :

_As user {mysapadm}_ for ASCS integration

[subs="specialchars,attributes,quotes,verbatim"]
----
# cd /usr/sap/<SID>/ASCS<instance number>/work
# ./sybdbfm.sap<SID>_ASCS<instance number> status

fault manager running, pid = 4118, fault manager overall status = OK, currently executing in mode PAUSING
*** sanity check report (65405)***.
node 1: server sapdb1, site FRA1.
db host status: OK.
db status OK hadr status PRIMARY.
node 2: server sapdb2, site FRA2.
db host status: OK.
db status OK hadr status STANDBY.
replication status: SYNC_OK.
failover prerequisites fulfilled: YES.
----

_As user {mysapadm}_ for standalone integration

[subs="specialchars,attributes,quotes,verbatim"]
----
# cd /usr/sap/{mySid}/{aseInst}/work
# ./sybdbfm status

fault manager running, pid = 4118, fault manager overall status = OK, currently executing in mode PAUSING
*** sanity check report (65405)***.
node 1: server sapdb1, site FRA1.
db host status: OK.
db status OK hadr status PRIMARY.
node 2: server sapdb2, site FRA2.
db host status: OK.
db status OK hadr status STANDBY.
replication status: SYNC_OK.
failover prerequisites fulfilled: YES.
----

Checking the log file is also a suitable method to validate the status.

_As user {mysapadm}_
//TODO: update output
[subs="specialchars,attributes"]
----
# cd /usr/sap/<SID>/ASCS<instance number>/work
# tail -f dev_sybdbfm
# ...

2020 02/28 15:34:30.523 (23234) ----- Log messages ----

2020 02/28 15:34:30.523 (23234) Info: saphostcontrol: Executing LiveDatabaseUpdate

2020 02/28 15:34:30.523 (23234) Info: saphostcontrol: LiveDatabaseUpdate successfully executed

2020 02/28 15:34:30.524 (23234) call is running.
2020 02/28 15:34:30.534 (23234) call exited (exit code 0).
2020 02/28 15:34:30.534 (23234) db status is:
 DB_OK.
2020 02/28 15:34:42.561 (23234) *** sanity check report (136)***.
2020 02/28 15:34:42.562 (23234) node 1: server <DB server1>, site <site name one>.
2020 02/28 15:34:42.562 (23234) db host status: OK.
2020 02/28 15:34:42.562 (23234) db status OK hadr status PRIMARY.
2020 02/28 15:34:42.562 (23234) node 2: server <DB server2>, site <site name two>.
2020 02/28 15:34:42.562 (23234) db host status: OK.
2020 02/28 15:34:42.562 (23234) db status OK hadr status STANDBY.
2020 02/28 15:34:42.562 (23234) replication status: SYNC_OK.
2020 02/28 15:34:57.688 (23234) *** sanity check report (137)***.
2020 02/28 15:34:57.688 (23234) node 1: server <DB server1>, site <site name one>.
2020 02/28 15:34:57.688 (23234) db host status: OK.
2020 02/28 15:34:57.688 (23234) db status OK hadr status PRIMARY.
2020 02/28 15:34:57.688 (23234) node 2: server <DB server2>, site <site name two>.
2020 02/28 15:34:57.688 (23234) db host status: OK.
2020 02/28 15:34:57.688 (23234) db status OK hadr status STANDBY.
2020 02/28 15:34:57.688 (23234) replication status: SYNC_OK.
2020 02/28 15:35:12.827 (23234) *** sanity check report (138)***.
2020 02/28 15:35:12.827 (23234) node 1: server <DB server1>, site <site name one>.
2020 02/28 15:35:12.827 (23234) db host status: OK.
2020 02/28 15:35:12.827 (23234) db status OK hadr status PRIMARY.
2020 02/28 15:35:12.827 (23234) node 2: server <DB server2>, site <site name two>.
2020 02/28 15:35:12.827 (23234) db host status: OK.
2020 02/28 15:35:12.827 (23234) db status OK hadr status STANDBY.
2020 02/28 15:35:12.827 (23234) replication status: SYNC_OK.
# ...
----
======================================

=== Cluster Integration of Fault Manager [[Cluster-Integration-of-Fault-Manager]]

We have *two options* to implement the FM in the pacemaker environment.

[discrete]
==== FM is part of the ASCS instance

image::sles4sap_nw740_cs_fm.svg[SVG]

[discrete]
==== FM is running as single instance (own SAP instance and cluster resource)
* additional configuration steps and resources are required (storage and IP)

image::sles4sap_nw740_cs+fm.svg[SVG]

[discrete]
==== FM is integrated as included service along with the ASCS
.Option one:
==============================================
The cluster configuration for the _primitive rsc_sap____<SID>____ASCS<instance number>_ have to be modified.
In the example we use:

- <SID> => {mySid}
- <instance number> => {myAscsIno}
- virtual hostname => {myVipNAscs}

[subs="specialchars,attributes,quotes,verbatim"]
----
# crm configure edit rsc_sap_{mySid}_ASCS{myAscsIno}
----

[subs="specialchars,attributes,quotes,verbatim"]
----
primitive rsc_sap_{mySid}_ASCS{myAscsIno} SAPInstance \
        operations $id=rsc_sap_{mySid}_ASCS{myAscsIno}-operations \
        op monitor interval=11 timeout=60 on-fail=restart \
        params InstanceName={mySid}_ASCS{myAscsIno}_{myVipNAscs} \
        START_PROFILE="/sapmnt/{mySid}/profile/{mySid}_ASCS{myAscsIno}_{myVipNAscs}" \
        AUTOMATIC_RECOVER=false MONITOR_SERVICES="sybdbfm|msg_server|enserver" \
        meta resource-stickiness=5000 failure-timeout=60 migration-threshold=1 priority=10
----
The FM service is not part of the default observed SAP instance services. If we specify the *MONITOR_SERVICES*
all default settings are overwritten by the named services. That means we have to count all services which are shown
as a result of the _sapcontrol -nr {myAscsIno} -function GetProcessList_ command. The example above is for an ENSA1 configuration.

NOTE: The cluster configuration is different for ENSA1 and ENS2 installation. The names for the MONITOR_SERVICES differ between this two versions.
==============================================

[discrete]
==== FM is running as single instance

The next steps may differ depending how the FM was installed before. In case of the FM was installed as integrated
service with the ASCS we must separate them first.

.For the example below we used this values:
* SID: {mySid}
  ** (the same SID of the SAP system where the DB is connected too simplifies the integration)
* instance number: {aseIno} (new)
* instance name: {aseInstN} (new)
* virtual IP(overlay IP address): {myVipAFM} (new)
* virtual hostname: {myVipNFM} (new)
* storage(NFS4 cloud storage): {myDevPartFM} (new)

[discrete]
==== FM separation procedure

* creating mount points on all cluster nodes
* maintain DNS (/etc/hosts)
* deactivate FM in the ASCS profile
* create a new profile for FM
* update the _/usr/sap/sapservices_
* copy the basic files for the initial start
* checking the FM is able to start

.Option two:
=============================================
Host preparation on all nodes which are cluster members.
[subs="specialchars,attributes"]
----
# mkdir -p /usr/sap/{mySid}/{aseInst}
## adding the vIP and the hostname of FM instance
# vi /etc/hosts
----

Only on one cluster node we must do the separation steps. If the FM is already running it needs to be stopped first.
FM configuration must be uncommented in the ASCS profile.
Edit the file _/usr/sap/{mySid}/SYS/profile/{mySid}____{myInstAscs}____{myVipNAscs}_ and uncomment the FM sections.

[subs="specialchars,attributes,quotes,verbatim"]
----
#-----------------------------------------------------------------------
# copy sybdbfm and dependent
#-----------------------------------------------------------------------
# _CP_SYBDBFM_ARG1 = list:$(DIR_CT_RUN)/instancedb.lst
# Execute_00 = immediate $(DIR_CT_RUN)/sapcpe$(FT_EXE) pf=$(_PF) $(_CP_SYBDBFM_ARG1)
# _CP_SYBDBFM_ARG2 = list:$(DIR_GLOBAL)/syb/linuxx86_64/cpe_sybodbc.lst
# _CP_SYBDBFM_ARG3 = source:$(DIR_GLOBAL)/syb/linuxx86_64/sybodbc
# Execute_01 = immediate $(DIR_CT_RUN)/sapcpe$(FT_EXE) pf=$(_PF) $(_CP_SYBDBFM_ARG2) $(_CP_SYBDBFM_ARG3)
# _CPARG1 = list:$(DIR_CT_RUN)/sapcrypto.lst
# Execute_02 = immediate $(DIR_CT_RUN)/sapcpe$(FT_EXE) pf=$(_PF) $(_CPARG1)
#-----------------------------------------------------------------------
# Start sybha
#-----------------------------------------------------------------------
# _SYBHAD = sybdbfm.sap$(SAPSYSTEMNAME)_$(INSTANCE_NAME)
# _SYBHA_PF = $(DIR_PROFILE)/SYBHA.PFL
# Execute_03 = local rm -f $(_SYBHAD)
# Execute_04 = local ln -s -f $(DIR_EXECUTABLE)/sybdbfm$(FT_EXE) $(_SYBHAD)
# Restart_Program_02 = local $(_SYBHAD) hadm pf=$(_SYBHA_PF)
----

Now we need an *new* instance profile for the FM. You can take a *copy* of the ASCS profile and adapt them carefully.

*The result should look like this:*
[subs="specialchars,attributes,quotes,verbatim"]
----
# cat /usr/sap/{mySid}/SYS/profile/{mySid}_{aseInst}_{myVipNFM}
SAPSYSTEMNAME = {mySid}
SAPSYSTEM = {aseIno}
INSTANCE_NAME = {aseInst}
DIR_CT_RUN = $(DIR_EXE_ROOT)$(DIR_SEP)$(OS_UNICODE)$(DIR_SEP)linuxx86_64
DIR_EXECUTABLE = $(DIR_INSTANCE)/exe
SAPLOCALHOST = {myVipNFM}
DIR_PROFILE = $(DIR_INSTALL)$(DIR_SEP)profile
_PF = $(DIR_PROFILE)/{mySid}_{aseInst}_{myVipNFM}
SETENV_00 = DIR_LIBRARY=$(DIR_LIBRARY)
SETENV_01 = LD_LIBRARY_PATH=$(DIR_LIBRARY):%(LD_LIBRARY_PATH)
SETENV_02 = SHLIB_PATH=$(DIR_LIBRARY):%(SHLIB_PATH)
SETENV_03 = LIBPATH=$(DIR_LIBRARY):%(LIBPATH)
SETENV_04 = PATH=$(DIR_EXECUTABLE):%(PATH)
SETENV_05 = SECUDIR=$(DIR_INSTANCE)/sec
#-----------------------------------------------------------------------
# copy sybdbfm and dependent
#-----------------------------------------------------------------------
_CP_SYBDBFM_ARG1 = list:$(DIR_CT_RUN)/instancedb.lst
Execute_00 = immediate $(DIR_CT_RUN)/sapcpe$(FT_EXE) pf=$(_PF) $(_CP_SYBDBFM_ARG1)
_CP_SYBDBFM_ARG2 = list:$(DIR_GLOBAL)/syb/linuxx86_64/cpe_sybodbc.lst
_CP_SYBDBFM_ARG3 = source:$(DIR_GLOBAL)/syb/linuxx86_64/sybodbc
Execute_01 = immediate $(DIR_CT_RUN)/sapcpe$(FT_EXE) pf=$(_PF) $(_CP_SYBDBFM_ARG2) $(_CP_SYBDBFM_ARG3)
_CPARG1 = list:$(DIR_CT_RUN)/sapcrypto.lst
Execute_02 = immediate $(DIR_CT_RUN)/sapcpe$(FT_EXE) pf=$(_PF) $(_CPARG1)
#-----------------------------------------------------------------------
# Start sybha
#-----------------------------------------------------------------------
_SYBHAD = sybdbfm.sap$(SAPSYSTEMNAME)_$(INSTANCE_NAME)
_SYBHA_PF = $(DIR_PROFILE)/SYBHA.PFL
Execute_03 = local rm -f $(_SYBHAD)
Execute_04 = local ln -s -f $(DIR_EXECUTABLE)/sybdbfm$(FT_EXE) $(_SYBHAD)
Restart_Program_02 = local $(_SYBHAD) hadm pf=$(_SYBHA_PF)
#suse cluster connector integration
service/halib = $(DIR_CT_RUN)/saphascriptco.so
service/halib_cluster_connector = /usr/bin/sap_suse_cluster_connector
----

The SAP sapstartsrv needs an entry in the _/usr/sap/sapservices_ for the FM. This must be done on *all* cluster nodes.
The ASCS entry can be used as template for the FM. The ERS entry is different and can't be used as a template.

[subs="specialchars,attributes,quotes,verbatim"]
----
# cat /usr/sap/sapservices
...
LD_LIBRARY_PATH=/usr/sap/{mySid}/{aseInst}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/{mySid}/{aseInst}/exe/sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{aseInst}_{myVipNFM} -D -u {mySapAdm}
----

Before we can test if FM is able to start as single instance we need some files.
[subs="specialchars,attributes,quotes,verbatim"]
----
# ip a a {myVipAFM} dev eth0
# mount {myDevPartFM} /usr/sap/{mySid}/{aseInst}
# mkdir -p /usr/sap/{mySid}/{aseInst}/{exe,work}
# chown -R {mySapAdm}.sapsys /usr/sap/{mySid}/{aseInst}
# cp -p /usr/sap/{mySid}/{{myInstAscs},{aseInst}}/exe/sapstartsrv
# cp -p /usr/sap/{mySid}/{{myInstAscs},{aseInst}}/exe/sapstart
# cp -p /usr/sap/{mySid}/{{myInstAscs},{aseInst}}/exe/libsapnwrfc.so
# cp -p /usr/sap/{mySid}/{myInstAscs}/exe/libicu* /usr/sap/{mySid}/{aseInst}/exe/
----

The new configuration can be tested as shown. Use CTRL+ c to stop it.
[subs="specialchars,attributes,quotes,verbatim"]
----
# LD_LIBRARY_PATH=/usr/sap/{mySid}/{aseInst}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH;
# /usr/sap/{mySid}/{aseInst}/exe/sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{aseInst}_{myVipNFM} -u {mySapAdm}

..
SAP Service SAP{mySid}_{aseIno} successfully started.
----

If the result *successfully started* is shown use Ctrl+c and interrupted the process. Now do the live
test with the sapstart framework. In any other cases check your log files e.g. _/usr/sap/{mySid}/{aseInst}/work_.

[subs="specialchars,attributes,quotes,verbatim"]
----
# sapcontrol -nr {aseIno} -function StartService {mySid}
# sapcontrol -nr {aseIno} -function Start
----

_As user {mysapadm}_ check if the _sybdbfm_ process is shown.

[subs="specialchars,attributes,quotes,verbatim"]
----
# sapcontrol -nr {aseIno} -function GetProcessList

23.02.2020 22:11:52
GetProcessList
OK
name, description, dispstatus, textstatus, starttime, elapsedtime, pid
sybdbfm, , GREEN, Running, 2020 04 22 18:28:31, 27:43:21, 17733
----

=============================================

.Cluster integration as independent instance
=============================================
Prepare a file which contains the resource for the FM. We are using the same method of three primitives
(IP, filesystem,SAP Instance) as it was done for the ASCS or ERS. The values must be adapted to your infrastructure.

[subs="specialchars,attributes,quotes,verbatim"]
----
# vi crm-fm.txt
primitive rsc_fs_{mySid}_{aseInst} Filesystem \
        params device="{myDevPartFM}" \
        directory="/usr/sap/{mySid}/{aseInst}" \
        fstype=nfs options="{mntopt}" \
        op start timeout=60s interval=0 \
        op stop timeout=60s interval=0 \
        op monitor interval=20s timeout=300s \
        meta target-role=Started
primitive rsc_ip_{mySid}_{aseInst} ocf:aliyun:vpc-move-ip \
        params address={myVipAFM} routing_table=vtb-gw8irrnvm8vd29iji5ufk interface=eth0 \
        op monitor interval=50s timeout=60s \
        meta target-role=Started
primitive rsc_sap_{mySid}_{aseInst} SAPInstance \
        operations $id=rsc_sap_{mySid}_{aseInst}-operations \
        op monitor interval=11 timeout=60 on-fail=restart \
        params InstanceName={mySid}_{aseInst}_myVipNFM \
        START_PROFILE="/sapmnt/{mySid}/profile/{mySid}_{aseInst}_myVipNFM" \
        AUTOMATIC_RECOVER=false MONITOR_SERVICES="sybdbfm" \
		meta priority=100 failure-timeout=60 migration-threshold=3 target-role=Started
group grp_{mySid}_{aseInst} rsc_ip_{mySid}_{aseInst} rsc_fs_{mySid}_{aseInst} rsc_sap_{mySid}_{aseInst}
----

.Upload the configuration to the cluster and check the cluster
[subs="specialchars,attributes,quotes,verbatim"]
----
# crm configure load update crm-fm.txt
# crm status
----
=============================================

=== Operating a Pacemaker-Controlled and FM-Monitored ASE Replication Setup

An ASE DB replication setup controlled by FM need some special rules which must be followed.
First of all how can the status be checked of the replication and FM itself. This chapter
will also give guidelines how to improve the takeover time and how to control such an environment.

.Checking the status of the database situation when FM is running together with ASCS
==========
//TODO: update output
Check the status and locate the actual primary DB host.

_As user {mysapadm}_ on the ASCS host
[subs="specialchars,attributes"]
----
# cd /usr/sap/<SID>/ASCS<instance nr>/work
# ./sybdbfm.sap<SID>_ASCS<instance nr> status
----

And check the log file _dev_sybdbfm_
[subs="specialchars,attributes"]
----
2020 03/28 19:38:52.200 (3290) *** sanity check report (2)***.
2020 03/28 19:38:52.200 (3290) node 1: server sapdb1, site FRA1.
2020 03/28 19:38:52.200 (3290) db host status: OK.
2020 03/28 19:38:52.200 (3290) db status OK hadr status STANDBY.
2020 03/28 19:38:52.200 (3290) node 2: server sapdb2, site FRA2.
2020 03/28 19:38:52.201 (3290) db host status: OK.
220 03/28 19:38:52.201 (3290) db status OK hadr status PRIMARY.
2020 03/28 19:38:52.201 (3290) replication status: SYNC_OK.
----

==========

//TODO: update output
.Checking the status of the database situation when FM is running as standalone instance
===============

Check the status and locate the actual primary DB host.

_As user {mysapadm}_ on the host where ever the FM is running
[subs="specialchars,attributes"]
----
# ssh {mysapadm}@{vfmhost}
# cd /usr/sap/{mySid}/{aseInst}/work
# ./sybdbfm.sap{mySid}_{aseInst} status
----

_As user root_ on the database host
[subs="specialchars,attributes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -user sapadm <secure password> -dbname {mySid} -dbtype syb -function GetDatabaseSystemStatus
# /usr/sap/hostctrl/exe/saphostctrl -user sapadm <secure password> -dbname {mySid} -dbtype syb -function GetDatabaseStatus
# /usr/sap/hostctrl/exe/saphostctrl -user sapadm <secure password> -dbname {mySid} -dbtype syb -function LiveDatabaseUpdate -updatemethod Check -updateoption TASK=REPLICATION_STATUS
----

_As user syb<sid>_ on the database host
[subs="specialchars,attributes"]
----
#  isql -UDR_admin -P <secure password> -S<db host>:4909 -X -w 1000
1> sap_status active_path
2> go
----

===============

The application server (PAS and AAS) environment must be adapted for the DB fail-over situation (takeover).
On each host which is providing a dialog server (PAS; AAS) the _.dbenv.sh_ and or _.dbenv.csh_ file needs to be extended.

.Modify the DB Environment Settings on the Dialog Server
===============

Add the missing value and extend the settings as below shown on each host who runs a dialog application server.
The names *server1* and *server2* specify the hostname of the DB host's where the DB can be run in active mode.

_As user {mysapadm}_
[subs="specialchars,attributes"]
----
# vi .dbenv.csh
...
setenv dbs_syb_server <server1:server2>
setenv dbs_syb_ha 1
...
----

_As user {mysapadm}_
[subs="specialchars,attributes"]
----
# vi .dbenv.sh
...
dbs_syb_server=<server1:server2>
export dbs_syb_server
dbs_syb_ha=1
export dbs_syb_ha
...
----

===============

IMPORTANT: The instance must be restarted to activate the changes.

.OS Settings for Faster Reaction Time After Primary DB Host is Down
=================

The default tcp_retries value is to high and causes a very long takeover time. With ASE16 PL7 the behavior is modified.
Up to this patch the change below improve the takeover time.

_As user root_
[subs="specialchars,attributes"]
----
# echo 3 >/proc/sys/net/ipv4/tcp_retries2
## makes the changes online
# vi /etc/sysctl.conf
...
net.ipv4.tcp_retries2 = 3
...
## makes the changes reboot persistent
----

=================


.Starting and Stopping The SAP System and Databases in Replication Mode
================

If Fault Manager is monitoring the Primary and Companion database and Fault Manager is monitored by
Pacemaker there is a special procedure necessary to start and stop the system.

.In general these steps are important to *start* the system:

//TODO: when should we start the FM as standalone instance?


* Start companion database + replication server
* Start primary database + replication server
* Change cluster maintenance mode to false
** Start ASCS with FM (automatic)
** Start ERS (automatic)
* Start PAS and AAS instances
* Optional: release cluster maintenance mode, if the SAP system was started manually
** File system must be mounted and IP must be set manually
** As user _<sid>adm_ with _sapcontrol -nr <instance number> -function StartSystem_



_As user root_ on companion database host
[subs="specialchars,attributes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -function StartDatabase -dbname <SID> -dbtype syb
# /usr/sap/hostctrl/exe/saphostctrl -function StartDatabase -dbname <SID>_REP -dbtype syb
----

_As user root_ on primary database host
[subs="specialchars,attributes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -function StartDatabase -dbname <SID> -dbtype syb
# /usr/sap/hostctrl/exe/saphostctrl -function StartDatabase -dbname <SID>_REP -dbtype syb
----

_As user root_ on one of the Pacemaker host for ASCS and ERS
[subs="specialchars,attributes"]
----
# crm configure property maintenance-mode=false
----

_As user <sid>adm_ on the host for PAS or AAS
[subs="specialchars,attributes"]
----
# sapcontrol -nr <instance number> -function StartSystem
----

NOTE: If the system should start one by one, use the command _sapcontrol -nr <instance number> -function StartSystem_. The direction must be: ASCS; ERS; PAS; AAS.

.In general these steps are important to *stop* the system:

//TODO: when should we stop the FM as standalone instance?

* Set cluster maintenance mode to _true_
* Stop PAS and AAS instances
* Stop ASCS with FM
* Stop ERS
* Stop primary database + replication server
* Stop companion database + replication server


_As user root_
[subs="specialchars,attributes"]
----
# crm configure property maintenance-mode=true
# crm status
----

_As user <sid>adm_ on one of the Pacemaker host for ASCS and ERS or PAS / AAS
[subs="specialchars,attributes"]
----
# sapcontrol -nr <instance number> -function StopSystem
----

NOTE: If the system should stop one by one, use the command _sapcontrol -nr <instance number> -function Stop_ on each instance host. The direction must be: AAS; PAS; ASCS; ERS.

_As user root_ on primary database host
[subs="specialchars,attributes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -function StopDatabase -dbname <SID> -dbtype syb
# /usr/sap/hostctrl/exe/saphostctrl -function StopDatabase -dbname <SID>_REP -dbtype syb
----

_As user root_ on companion database host
[subs="specialchars,attributes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -function StopDatabase -dbname <SID> -dbtype syb
# /usr/sap/hostctrl/exe/saphostctrl -function StopDatabase -dbname <SID>_REP -dbtype syb
----

IMPORTANT: The Pacemaker-controlled server must be stopped in a proper way, too. Depending
on the stonith method which is implemented different procedures are available.

_As user root_ on one cluster node
[subs="specialchars,attributes"]
----
# crm cluster run "crm cluster stop"
----

_As user root_ on each node
[subs="specialchars,attributes"]
----
# reboot
## or
# poweroff
----

================

==== Testing the Replication and FM Cluster Integration

Important for each high availability solution is an extensive testing procedure. That ensures that the solution is working as expected in case of a failure.

.Triggering a Database fail-over and Monitoring if FM Is Working
================

Check the status and locate the primary site.
_As user {mysapadm}_ on the ASCS host
[subs="specialchars,attributes"]
----
# cd /usr/sap/<SID>/ASCS<instance nr>/work
# ./sybdbfm.sap<SID>_ASCS<instance nr> status
----

And check the log file _dev_sybdbfm_
[subs="specialchars,attributes"]
----
2020 03/28 19:38:52.200 (3290) *** sanity check report (2)***.
2020 03/28 19:38:52.200 (3290) node 1: server sapdb1, site FRA1.
2020 03/28 19:38:52.200 (3290) db host status: OK.
2020 03/28 19:38:52.200 (3290) db status OK hadr status STANDBY.
2020 03/28 19:38:52.200 (3290) node 2: server sapdb2, site FRA2.
2020 03/28 19:38:52.201 (3290) db host status: OK.
2020 03/28 19:38:52.201 (3290) db status OK hadr status PRIMARY.
2020 03/28 19:38:52.201 (3290) replication status: SYNC_OK.
----

* Now destroy the primary database server.
* Monitor the takeover process with the FM.

_As user {mysapadm}_ on the ASCS host (FM running as integrated ASCS service)
[subs="specialchars,attributes"]
----
# cd /usr/sap/<SID>/ASCS<instance nr>/work
# tail -f  dev_sybdbfm
----
================

.Selected Output From the Takeover Process.
=========

[subs="specialchars,attributes,quotes,verbatim"]
-----
...
    2020 03/2711:08:38.301 (3290) ** *** sanity check report (270)*** **.
    2020 03/2711:08:38.301 (3290) node 1: server sapdb1, site FRA1.
    2020 03/2711:08:38.301 (3290) db host status: OK.
    2020 03/2711:08:38.301 (3290) db status OK hadr status STANDBY.
    2020 03/2711:08:38.301 (3290) node 2: server sapdb2, site FRA2.
    2020 03/2711:08:38.301 (3290) db host status: OK.
    2020 03/2711:08:38.301 (3290) db status OK hadr status PRIMARY.
    2020 03/2711:08:38.301 (3290) replication status: SYNC_OK.
    2020 03/2711:08:50.416 (3290) ERROR in function SimpleFetch (1832) (SQLExecDirect failed): (30046) [08S01] [SAP][ASE ODBC Driver]Connection to the server has been lost. Unresponsive Connection was disconnected during command timeout. Check the server to determine the status of any open transactions.
    2020 03/2711:08:50.416 (3290) ERROR in function SimpleFetch (1832) (SQLExecDirect failed): (30149) [HYT00] [SAP][ASE ODBC Driver]The command has timed out.
    2020 03/2711:08:50.416 (3290) execution of statement master..sp_hadr_admin get_request, '1' failed.
    2020 03/2711:08:50.416 (3290) ERROR in function SimpleFetch (1824) (SQLAllocStmt failed): (30102) [HY010] [SAP][ASE ODBC Driver]Function sequence error
    2020 03/2711:08:50.416 (3290) execution of statement select top 1 convert( varchar(10), @@hadr_mode ) || ' ' || convert( varchar(10), @@hadr_state ) from sysobjects failed.
    2020 03/2711:08:50.416 (3290) disconnect connection
    2020 03/2711:09:22.505 (3290) ERROR in function SQLConnectWithRetry (1341) (SQLConnectWithRetry failed): (30293) [HY000] [SAP][ASE ODBC Driver]The socket failed to connect within the timeout specified.
    2020 03/2711:09:22.505 (3290) ERROR in function SQLConnectWithRetry (1341) (SQLConnectWithRetry failed): (30012) [08001] [SAP][ASE ODBC Driver]Client unable to establish a connection
    2020 03/2711:09:22.505 (3290) connected with warnings (555E69805100)
    2020 03/2711:09:22.505 (3290) ERROR in function SimpleFetch (1824) (SQLAllocStmt failed): (30293) [HY000] [SAP][ASE ODBC Driver]The socket failed to connect within the timeout specified.
    2020 03/2711:09:22.505 (3290) ERROR in function SimpleFetch (1824) (SQLAllocStmt failed): (30012) [08001] [SAP][ASE ODBC Driver]Client unable to establish a connection
    2020 03/2711:09:22.505 (3290) execution of statement select top 1 convert( varchar(10), @@hadr_mode ) || ' ' || convert( varchar(10), @@hadr_state ) from sysobjects failed.
    2020 03/2711:09:22.505 (3290) disconnect connection
    2020 03/2711:09:22.505 (3290) primary site unusable.
...
    2020 03/2711:09:22.984 (3290) primary site unusable.
    2020 03/2711:09:22.984 (3290) ** *** sanity check report (271)*** **.
    2020 03/2711:09:22.984 (3290) node 1: server sapdb1, site FRA1.
    2020 03/2711:09:22.984 (3290) db host status: OK.
    2020 03/2711:09:22.984 (3290) db status OK hadr status STANDBY.
    2020 03/2711:09:22.984 (3290) node 2: server sapdb2, site FRA2.
    2020 03/2711:09:22.984 (3290) db host status: UNUSABLE.
    2020 03/2711:09:22.984 (3290) db status DB INDOUBT hadr status UNREACHABLE.
    2020 03/2711:09:22.984 (3290) replication status: SYNC_OK.
    2020 03/2711:09:23.047 (3290) doAction: Primary database is declared dead or unusable.
    2020 03/2711:09:23.047 (3290) disconnect connection
    2020 03/2711:09:23.047 (3290) database host cannot be reached.
    **2020 03/2711:09:23.047 (3290) doAction: fail-over.**
...
    2020 03/2711:11:55.497 (3290) ** *** sanity check report (273)*** **.
    2020 03/2711:11:55.497 (3290) node 1: server sapdb1, site FRA1.
    2020 03/2711:11:55.497 (3290) db host status: OK.
    **2020 03/2711:11:55.497 (3290) db status OK hadr status PRIMARY.**
    2020 03/2711:11:55.497 (3290) node 2: server sapdb2, site FRA2.
    2020 03/2711:11:55.497 (3290) db host status: UNUSABLE.
    2020 03/2711:11:55.498 (3290) db status DB INDOUBT hadr status UNREACHABLE.
    2020 03/2711:11:55.498 (3290) replication status: UNKNOWN.
    2020 03/2711:11:55.555 (3290) doAction: Standby database is declared dead or unusable.
    2020 03/2711:11:55.555 (3290) disconnect connection
    **2020 03/2711:11:55.555 (3290) doAction: Companion db host is declared unusable.**
    2020 03/2711:11:55.555 (3290) doAction: no action defined.
    2020 03/2711:11:58.568 (3290) Error: NIECONN_REFUSED (No route to host), NiRawConnect failed in plugin_fopen()
...
############ host is coming back online ################
    2020 03/2711:18:45.579 (3290) call is running.
    2020 03/2711:18:45.589 (3290) call exited (exit code 0).
    2020 03/2711:18:45.589 (3290) db status is: DB_OK.
    2020 03/2711:18:45.589 (3290) doAction: Standby database is declared dead or unusable.
    2020 03/2711:18:45.589 (3290) disconnect connection
    **2020 03/2711:18:45.589 (3290) doAction: Companion db host is declared ok.**
    **2020 03/2711:18:45.589 (3290) doAction: restart database.**
    2020 03/2711:18:45.805 (3290) Webmethod returned successfully
...
    2020 03/2711:22:43.677 (3290) ** *** sanity check report (286)*** **.
    2020 03/2711:22:43.677 (3290) node 1: server sapdb1, site FRA1.
    2020 03/2711:22:43.677 (3290) db host status: OK.
    2020 03/2711:22:43.677 (3290) db status OK hadr status PRIMARY.
    2020 03/2711:22:43.677 (3290) node 2: server sapdb2, site FRA2.
    2020 03/2711:22:43.677 (3290) db host status: OK.
    **2020 03/2711:22:43.677 (3290) db status OK hadr status STANDBY.**
    **2020 03/2711:22:43.677 (3290) replication status: SYNC_OK.**
...
-----

_As user root_
[subs="specialchars,attributes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -user sapadm <secure password> -dbname <SID> -dbtype syb -function LiveDatabaseUpdate -updatemethod Check -updateoption TASK=REPLICATION_STATUS

Webmethod returned successfully
Operation ID: 5254001F87CB1C75B5C34755C991EDFA

----- Response data ----
TASK_NAME=REPLICATION_STATUS
REPLICATION_STATUS=active
PRIMARY_SITE=<site1>
STANDBY_SITE=<site2>
REPLICATION_MODE=sync
ASE transaction log backlog (MB)=0
Replication queue backlog (MB)=0
TASK_STATUS=OK
----- Log messages ----
Info: saphostcontrol: Executing LiveDatabaseUpdate
Info: saphostcontrol: LiveDatabaseUpdate successfully executed
----

=========

.Triggering an FM Failure
================

Killing the FM process more than five times will bring pacemaker in action. Up to five times the saphostagent will take care of the SAP process. If this fail-count is reached in a specific time window, the service will not be restarted.

_As user {mysapadm}_
[subs="specialchars,attributes"]
----
# pkill -9 sybdbfm
## check that the PID has changed
# sapcontrol -nr {aseIno} -function GetProcessList
# pkill -9 sybdbfm
...
# sapcontrol -nr {aseIno} -function GetProcessList
...
sybdbfm, , GRAY, Stopped, , , 11154
...
----

Now pacemaker will restart the FM instance locally first.
_As user root_
[subs="specialchars,attributes"]
----
# crm_mon -1rfn
...
Migration Summary:
* Node <hostname>:
rsc_sap_{mySid}_{aseInst}: migration-threshold=3 fail-count=1 last-failure='Fri Mar 27 13:46:39 2020
...
----

NOTE: If the *fail-count* reaches the defined threshold, the FM instance is moved away from that host. If the FM is integrated as part of the ASCS both will be moved away.

================

== References

For more information, see the documents listed below.

=== Pacemaker

- Pacemaker 1.1 Configuration Explained:
https://clusterlabs.org/pacemaker/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/

:leveloffset: 2
include::SAPNotes_ha740.adoc[]

++++
<?pdfpagebreak?>
++++

////
############################
#
# APENDIX
#
############################
////

:leveloffset: 0

== Appendix

=== CRM Configuration

The complete crm configuratoin for {sap} system {mySid} looks as follows:

[subs="attributes"]
----
## nodes

node 1084753931: {myNode1}
node 1084753932: {myNode2}

## primitives for ASCS and ERS

primitive rsc_fs_{mySID}_{myInstAscs} Filesystem \
	params device="{myDevPartAscs}" directory="/usr/sap/{mySID}/{myInstAscs}" fstype=xfs \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=40s
primitive rsc_fs_{mySID}_{myInstErs} Filesystem \
	params device="{myDevPartErs}" directory="/usr/sap/{mySID}/{myInstErs}" fstype=xfs \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=40s
primitive rsc_ip_{mySID}_{myInstAscs} IPaddr2 \
	params ip={myVipAAscs} \
	op monitor interval=10s timeout=20s
primitive rsc_ip_{mySID}_{myInstErs} IPaddr2 \
	params ip={myVipAErs} \
	op monitor interval=10s timeout=20s
primitive rsc_sap_{mySID}_{myInstAscs} SAPInstance \
	operations $id=rsc_sap_{mySID}_{myInstAscs}-operations \
	op monitor interval=11 timeout=60 on-fail=restart \
	params InstanceName={mySID}_{myInstAscs}_{myVipNAscs} \
	 START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstAscs}_{myVipNAscs}" \
	 AUTOMATIC_RECOVER=false \
	meta resource-stickiness=5000 failure-timeout=60 migration-threshold=1 \
	 priority=10
primitive rsc_sap_{mySID}_{myInstErs} SAPInstance \
	operations $id=rsc_sap_{mySID}_{myInstErs}-operations \
	op monitor interval=11 timeout=60 on-fail=restart \
	params InstanceName={mySID}_{myInstErs}_{myVipNErs} \
	 START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstErs}_{myVipNErs}" \
	 AUTOMATIC_RECOVER=false IS_ERS=true \
	meta priority=1000
primitive stonith-sbd stonith:external/sbd \
	params pcmk_delay_max=30s

## group definitions for ASCS and ERS

group grp_{mySID}_{myInstAscs} rsc_ip_{mySID}_{myInstAscs} rsc_fs_{mySID}_{myInstAscs} rsc_sap_{mySID}_{myInstAscs} \
	meta resource-stickiness=3000
group grp_{mySID}_{myInstErs} rsc_ip_{mySID}_{myInstErs} rsc_fs_{mySID}_{myInstErs} rsc_sap_{mySID}_{myInstErs}

## constraints between ASCS and ERS

colocation col_sap_{mySid}_not_both -5000: grp_{mySID}_{myInstErs} grp_{mySID}_{myInstAscs}
location loc_sap_{mySid}_fail-over_to_ers rsc_sap_{mySID}_{myInstAscs} \
	rule 2000: runs_ers_{mySID} eq 1
order ord_sap_{mySid}_first_ascs Optional: rsc_sap_{mySID}_{myInstAscs}:start rsc_sap_{mySID}_{myInstErs}:stop symmetrical=false

## crm properties and more

property cib-bootstrap-options: \
	have-watchdog=true \
	dc-version=1.1.19-20181105.ccd6b5b10 \
	cluster-infrastructure=corosync \
	cluster-name=hacluster \
	stonith-enabled=true \
	last-lrm-refresh=1494346532
rsc_defaults rsc-options: \
	resource-stickiness=1 \
	migration-threshold=3
op_defaults op-options: \
	timeout=600 \
	record-pending=true
----

=== Corosync Configuration of the Two Node Cluster

Find below the corosync configuration including a secondary heartbeat ring.

[subs="specialchars,attributes"]
----
# cat /etc/corosync/corosync.conf
# Please read the corosync.conf.5 manual page
totem {
    version: 2
	rrp_mode: passive
    secauth: on
    crypto_hash: sha1
    crypto_cipher: aes256
    cluster_name: hacluster
    clear_node_high_bit: yes
    token: 5000
    token_retransmits_before_loss_const: 10
    join: 60
    consensus: 6000
    max_messages: 20
    interface {
        ringnumber: 0
        mcastport: 5405
        ttl: 1
    }

    transport: udpu
}

logging {
    fileline: off
    to_stderr: no
    to_logfile: no
    logfile: /var/log/cluster/corosync.log
    to_syslog: yes
    debug: off
    timestamp: on
    logger_subsys {
        subsys: QUORUM
        debug: off
    }

}

nodelist {
    node {
        ring0_addr: {myIP2nd1}
		ring1_addr: {myIP2nd1r2}
        nodeid: 1
    }

    node {
        ring0_addr: {myIP2nd2}
		ring1_addr: {myIP2nd2r2}
        nodeid: 2
    }

}

quorum {

    # Enable and configure quorum subsystem (default: off)
    # see also corosync.conf.5 and votequorum.5
    provider: corosync_votequorum
    expected_votes: 2
    two_node: 1
}
----

=== Corosync Configuration of the multi Node Cluster

[subs="specialchars,attributes,quotes"]
----
	# Please read the corosync.conf.5 manual page
	totem {
		version: 2
		secauth: on
		crypto_hash: sha1
		crypto_cipher: aes256
		cluster_name: hacluster
		clear_node_high_bit: yes
		token: 5000
		token_retransmits_before_loss_const: 10
		join: 60
		consensus: 6000
		max_messages: 20
		interface {
			ringnumber: 0
			mcastport: 5405
			ttl: 1
		}

		transport: udpu
	}

	logging {
		fileline: off
		to_stderr: no
		to_logfile: no
		logfile: /var/log/cluster/corosync.log
		to_syslog: yes
		debug: off
		timestamp: on
		logger_subsys {
			subsys: QUORUM
			debug: off
		}

	}

	nodelist {
		node {
			ring0_addr: {myIP3nd1}
			nodeid: 1
		}

		node {
			ring0_addr: {myIP3nd2}
			nodeid: 2
		}

		node {
			ring0_addr: {myIP3nd3}
			nodeid: 3
		}

	}

	quorum {

		# Enable and configure quorum subsystem (default: off)
		# see also corosync.conf.5 and votequorum.5
		provider: corosync_votequorum
		expected_votes: 3
		two_node: 0
	}
----

// Standard SUSE Best Practices includes
== Legal Notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

////
Version 1.0 - Initial version based on SLE12 and NW 7.40
Version 1.1 - Including SLE15 preparation, NW 7.50, Unicast set-up
////
//
// REVISION 1.1  2018/04
//   - PowerLE
//   - corr: StartService
//   - removed no-quorum-policy=ignore
//   - corr: upper/lowercase of section titles
// Revision 1.2 2020/04
//   - adapting to AliCloud and adding ASE replication
